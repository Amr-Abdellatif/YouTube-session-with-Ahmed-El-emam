{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a54518c-c197-41d6-abd4-a2df08abc8b6",
   "metadata": {},
   "source": [
    "# Lets start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca6d141",
   "metadata": {},
   "source": [
    "### Important links\n",
    "1. https://www.cbe.org.eg/ar/economic-research/economic-reports\n",
    "2. https://www.pif.gov.sa/en/investors/annual-reports/\n",
    "3. https://www.pif.gov.sa/en/investors/\n",
    "4. https://www.pif.gov.sa/en/investors/credit-rating/ -> a perfect use case  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d8349",
   "metadata": {},
   "source": [
    "##### https://www.cbe.org.eg/ar/economic-research/economic-reports/annual-report -> cbe image pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f5bbaa-f599-4994-8cb3-145d2d5ca983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Mon_Apr__3_17:16:06_PDT_2023\n",
      "Cuda compilation tools, release 12.1, V12.1.105\n",
      "Build cuda_12.1.r12.1/compiler.32688072_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n",
    "!echo $CUDA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d794211d-6229-4b29-9564-4fc8212dc464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting jinja2\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting networkx\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Collecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting triton==3.2.0\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting numpy\n",
      "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pillow-11.0.0 sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47a907-4969-4a10-b249-1c18bfe2048a",
   "metadata": {},
   "source": [
    "# Installing what we need for qwen2.5-vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3439816f-7517-46bc-a108-e7e7b62d824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-p0k63ul4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-p0k63ul4\n",
      "  Resolved https://github.com/huggingface/transformers to commit 94ae1ba5b55e79ba766582de8a199d8ccf24a021\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "donetting requirements to build wheel ... \u001b[?25l\n",
      "doneeparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /venv/main/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.28.1)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers==4.50.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.10/site-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.50.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: qwen-vl-utils[decord]==0.0.8 in /venv/main/lib/python3.10/site-packages (0.0.8)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from qwen-vl-utils[decord]==0.0.8) (2.32.3)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.10/site-packages (from qwen-vl-utils[decord]==0.0.8) (24.2)\n",
      "Requirement already satisfied: av in /venv/main/lib/python3.10/site-packages (from qwen-vl-utils[decord]==0.0.8) (14.2.0)\n",
      "Requirement already satisfied: pillow in /venv/main/lib/python3.10/site-packages (from qwen-vl-utils[decord]==0.0.8) (11.0.0)\n",
      "Requirement already satisfied: decord in /venv/main/lib/python3.10/site-packages (from qwen-vl-utils[decord]==0.0.8) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /venv/main/lib/python3.10/site-packages (from decord->qwen-vl-utils[decord]==0.0.8) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->qwen-vl-utils[decord]==0.0.8) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->qwen-vl-utils[decord]==0.0.8) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->qwen-vl-utils[decord]==0.0.8) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->qwen-vl-utils[decord]==0.0.8) (2.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.10/site-packages (0.45.3)\n",
      "Requirement already satisfied: flash-attn in /venv/main/lib/python3.10/site-packages (2.7.4.post1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /venv/main/lib/python3.10/site-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: einops in /venv/main/lib/python3.10/site-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers accelerate \n",
    "!pip install qwen-vl-utils[decord]==0.0.8 \n",
    "!pip install bitsandbytes flash-attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76c2e55-e725-4e77-ba3a-3ce3c1b0897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "Current CUDA device: 0\n",
      "Device name: NVIDIA GeForce RTX 4090\n",
      "Device count: 1\n",
      "flash_attn package is installed: version 2.7.4.post1\n",
      "\n",
      "GPU Memory Information:\n",
      "Total memory: 25.39 GB\n",
      "Allocated memory: 0.00 GB\n",
      "Cached memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "def check_environment():\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "        # Alternative check for flash attention\n",
    "        try:\n",
    "            import flash_attn\n",
    "            print(f\"flash_attn package is installed: version {flash_attn.__version__}\")\n",
    "        except ImportError:\n",
    "            print(\"flash_attn package is not installed\")\n",
    "    \n",
    "    # Memory info if CUDA is available\n",
    "    if cuda_available:\n",
    "        print(\"\\nGPU Memory Information:\")\n",
    "        print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "        print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e32a013a-2fce-4834-b529-9ade93ed7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface-cli login --token <token> -> Terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf0fdb",
   "metadata": {},
   "source": [
    "### Model card on HF\n",
    "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd207a4-4b06-4e05-9554-26cb35f7a791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a18c1f22fb42968f5455cec643bb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from typing import List, Union, Dict\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class QwenVLProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "        min_pixels: int = 128*16*16,\n",
    "        max_pixels: int = 1024*16*16,\n",
    "        cache_dir: str = None  # Add cache_dir parameter\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the QwenVL processor with custom configuration.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name or path of the model to load\n",
    "            device: Device to run the model on ('cuda' or 'cpu')\n",
    "            use_flash_attention: Whether to use flash attention\n",
    "            min_pixels: Minimum number of pixels for image processing\n",
    "            max_pixels: Maximum number of pixels for image processing\n",
    "        \"\"\"\n",
    "        # Configure CUDA memory allocation\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Load model and assign to self\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\", \n",
    "            use_cache=True,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # Load processor and assign to self\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            min_pixels=min_pixels,\n",
    "            max_pixels=max_pixels,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def _encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode a local image file to base64.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the local image file\n",
    "\n",
    "        Returns:\n",
    "            Base64 encoded string of the image\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def prepare_messages(\n",
    "        self,\n",
    "        image_paths: Union[str, List[str]],\n",
    "        prompt: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare messages for the model using local image paths.\n",
    "\n",
    "        Args:\n",
    "            image_paths: Single path or list of paths to local images\n",
    "            prompt: Text prompt to process with the images\n",
    "\n",
    "        Returns:\n",
    "            List of formatted messages for the model\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "\n",
    "        messages = []\n",
    "        for path in image_paths:\n",
    "            encoded_image = self._encode_image(path)\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": encoded_image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    def process_images(\n",
    "        self,\n",
    "        image_paths: Union[str, List[str]],\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 2000,\n",
    "        temperature: float = 0.01,\n",
    "        top_p: float = 0.9 # creates a smaller pool of probably avaliable words\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process local images with the given prompt.\n",
    "\n",
    "        Args:\n",
    "            image_paths: Single path or list of paths to local images\n",
    "            prompt: Text prompt to process with the images\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            top_p: Top-p sampling parameter\n",
    "\n",
    "        Returns:\n",
    "            List of generated responses for each image\n",
    "        \"\"\"\n",
    "        messages = self.prepare_messages(image_paths, prompt)\n",
    "\n",
    "        with torch.inference_mode(): # check pytorch autograd mechanics page\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # put the inputs on device\n",
    "            inputs = inputs.to(self.device) \n",
    "\n",
    "            \n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "\n",
    "        return output_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    processor = QwenVLProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3128675-3587-408d-8568-12d7c867dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: The image contains a handwritten quote on lined notebook paper. The text reads:\n",
      "\n",
      "\"‘Don’t ever let someone tell you, you can’t do something. Not even me. You got a dream, you got to protect it. People can’t do something themselves, they want to tell you, you can’t do it. You want something, go get it, period.\n",
      "All right?’\n",
      "- From Pursuit of Happiness\"\n",
      "\n",
      "The quote is attributed to the movie \"Pursuit of Happiness.\" The handwriting appears neat and legible, with the lines of the notebook paper providing a structured background for the text.\n",
      "time is : 3.92 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Process single image\n",
    "image_path = \"./image.jpeg\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format.\"\"\"\n",
    ")\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "\n",
    "end_time = time.time() - start_time \n",
    "print(f\"time is : {end_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9c51a-dc80-4f52-9106-1dc16f4e7948",
   "metadata": {},
   "source": [
    "# Lets see some English images examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1f3bc-328b-444b-83d9-e263650612ee",
   "metadata": {},
   "source": [
    "## Visualizations interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c911182-68f3-4afb-b125-f5a320b0f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: The image is a bar chart titled \"Fintech Market Growth Projections.\" The chart shows the projected market valuation of the fintech industry based on Compound Annual Growth Rate (CAGR) from 2021 to 2029.\n",
      "\n",
      "### Key Details:\n",
      "- **Title**: Fintech Market Growth Projections\n",
      "- **Y-Axis**: Market Valuation Calculated Based on CAGR\n",
      "- **X-Axis**: Year (from 2021 to 2029)\n",
      "- **Data Points**:\n",
      "  - **2021**: Approximately $100B\n",
      "  - **2022**: Approximately $120B\n",
      "  - **2023**: Approximately $140B\n",
      "  - **2024**: Approximately $180B\n",
      "  - **2025**: Approximately $260B\n",
      "  - **2026**: Approximately $320B\n",
      "  - **2027**: Approximately $400B\n",
      "  - **2028**: Approximately $500B\n",
      "  - **2029**: Approximately $620B\n",
      "\n",
      "The bars increase steadily each year, indicating a consistent growth trend in the fintech market valuation over the period shown.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./visualization_eng.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format.\"\"\"\n",
    ")\n",
    "print(f\"Single image result: {result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecabcbb-f71b-494f-9f3e-d90616a0be58",
   "metadata": {},
   "source": [
    "## prompting can help .. the more specific you are the more accurate results you get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d290b16-2988-4f73-9c0c-455a17a4dac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: Certainly! Below is the detailed information extracted from the provided image, organized into sections:\n",
      "\n",
      "---\n",
      "\n",
      "### **PIF Vision Realization Program**\n",
      "\n",
      "#### **Introduction**\n",
      "The Public Investment Fund (PIF) has committed to Vision 2030, with a focus on driving sustainable and transformative economic change through diversification of the Saudi Arabian economy and building its international asset portfolio. The program aims at generating sustainable returns and fostering economic diversification.\n",
      "\n",
      "#### **Strategic Review | PIF Vision Realization Program**\n",
      "\n",
      "#### **In 2021, PIF Launched Its Vision Realization Program 2021-2025:**\n",
      "- **Vision:** The program is designed to guide the Fund's evolution and align the Fund's strategy with the Kingdom's vision and ambitions.\n",
      "- **Objectives:** The VIP emphasizes PIF's role in achieving Vision 2030 by building a diversified and sustainable future, positioning the Fund as a practical player in Saudi Arabia's broader economic narrative.\n",
      "\n",
      "#### **Source of Funding**\n",
      "- **Capital Injections from the Government**\n",
      "- **Government assets transferred to PIF**\n",
      "- **Capital markets**\n",
      "- **Retained earnings from investments**\n",
      "\n",
      "#### **Direct Objectives**\n",
      "- **Cover the assets of PIF**\n",
      "- **Build strategic economic partnerships through PIF**\n",
      "- **Locate cutting-edge technology and knowledge through PIF**\n",
      "\n",
      "#### **Strategic Pillars**\n",
      "- **Launch and grow domestic sectors**\n",
      "- **Develop domestic real estate projects**\n",
      "- **Develop mega-projects**\n",
      "- **Create value for PIF’s equity assets internationally**\n",
      "- **Support economic development and enable Vision 2030**\n",
      "- **Exploit portfolio synergies and create long-term shareholder value**\n",
      "- **Increase funding and strengthen PIF’s balance sheet**\n",
      "- **Strengthen PIF as an institution**\n",
      "\n",
      "#### **Expected Impact by 2025**\n",
      "- **Cumulative Non-of GDP Contribution:** SAR 1.2 TN\n",
      "- **Job Creation:** 1.8 MN (Direct, Indirect and Induced jobs)\n",
      "- **Contribution to Local Content:** 60%\n",
      "- **Cumulative Non-governmental Investments:** SAR 1.2 TN (Includes domestic and foreign direct investment)\n",
      "\n",
      "#### **13 Strategic Sectors of Focus**\n",
      "1. Aerospace and Defense\n",
      "2. Real Estate\n",
      "3. Metals and Mining\n",
      "4. Consumer Goods and Retail\n",
      "5. Automotive\n",
      "6. Entertainment, Leisure and Sports\n",
      "7. Utilities and Renewables\n",
      "8. Financial Services\n",
      "9. Building Constructions, Materials and Services\n",
      "10. Transport and Logistics\n",
      "11. Health Care\n",
      "12. Food and Agriculture\n",
      "13. Telecom, Media and Technology\n",
      "\n",
      "#### **2025 Targets**\n",
      "- **Assets Under Management (AUM):** SAR 4 TN (Baseline: SAR 2.9 TN in 2020)\n",
      "- **Share of PIF Assets in New Sectors:** 21% (Baseline: 10% in 2020)\n",
      "- **New Local Investments:** SAR 150 BN (Annual Minimum)\n",
      "- **Share of PIF Assets in International Sectors:** 24% (Baseline: 10% in 2020)\n",
      "\n",
      "---\n",
      "\n",
      "This summary captures all the key points and targets outlined in the image. If you need further clarification or specific details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./pif.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format.\"\"\"\n",
    ")\n",
    "print(f\"Single image result: {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89758c1d-24d9-419a-a5cf-16907a3ee449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: Here is the extracted text and numbers from the image:\n",
      "\n",
      "**PIF VISION REALIZATION PROGRAM**\n",
      "\n",
      "- **Strategic Review | PIF Vision Realization Program**\n",
      "- **The Public Investment Fund's cornerstone in Vision 2030, tasked with driving sustainable and transformative economic change through investments in the Saudi economy and building its international asset portfolio, to achieve long-term sustainable returns and fostering economic diversification.**\n",
      "- **In 2021, PIF launched its Vision Realization Program 2021-2025, redefining a critical step in the Fund’s evolution and the alignment of its strategy with the Kingdom’s objectives and ambitions. The VIP emphasizes PIF’s role in achieving Vision 2030 by building a diversified and sustainable future, positioning the Fund as a practical player in Saudi Arabia’s broader economic narrative.**\n",
      "- **EXPECTED IMPACT BY 2025**\n",
      "  - **Cumulative Non-of GDP Contribution:** SAR 1.2 TN\n",
      "  - **Job Creation:** 1.8 MN (Direct, Indirect and Induced jobs)\n",
      "  - **Contribution to Local Content:** 60%\n",
      "  - **Cumulative Non-governmental Investments:** SAR 1.2 TN (Includes domestic and foreign direct investment)\n",
      "  - **13 STRATEGIC SECTORS OF FOCUS:**\n",
      "    - Aerospace and Defense\n",
      "    - Real Estate\n",
      "    - Metals and Mining\n",
      "    - Consumer Goods and Retail\n",
      "    - Automotive\n",
      "    - Entertainment, Leisure and Sports\n",
      "    - Utilities and Renewables\n",
      "    - Financial Services\n",
      "    - Building Constructions, Materials and Services\n",
      "    - Transport and Logistics\n",
      "    - Health Care\n",
      "    - Food and Agriculture\n",
      "    - Telecom, Media and Technology\n",
      "- **SOURCE OF FUNDING:**\n",
      "  - Capital injections from the government\n",
      "  - Government assets transferred to PIF\n",
      "  - Capital markets\n",
      "  - Retained earnings from investments\n",
      "- **DIRECT OBJECTIVES:**\n",
      "  - Grow the assets of PIF\n",
      "  - Build strategic economic partnerships through PIF\n",
      "  - Locate cutting-edge technology and knowledge through PIF\n",
      "- **STRATEGIC PILLARS:**\n",
      "  - Launch and grow domestic sectors\n",
      "  - Develop domestic real estate projects\n",
      "  - Develop mega-projects\n",
      "  - Capitalize on PIF’s assets internationally\n",
      "  - Support economic development and enable Vision 2030\n",
      "  - Exploit portfolio synergies and create value for shareholders\n",
      "  - Ensure funding and strengthen PIF’s balance sheet\n",
      "  - Strengthen PIF as an institution\n",
      "- **2025 TARGETS**\n",
      "  - **Assets Under Management (AUM):** SAR 4 TN (Baseline: SAR 2.9 TN in 2020)\n",
      "  - **Share of PIF Assets in New Sectors:** 21% (Baseline: 10% in 2020)\n",
      "  - **New Local Investments:** SAR 150 BN Annual Minimum\n",
      "  - **Share of PIF Assets in International Sectors:** 24% (Baseline: 10% in 2020)\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./pif.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. just extract the text / numbers you see .\"\"\"\n",
    ")\n",
    "print(f\"Single image result: {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426b8068-656c-4aec-8370-f68951deec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: The page numbers in the image are 26 and 27.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./pif.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"Extract the page numbers.\"\"\"\n",
    ")\n",
    "print(f\"Single image result: {result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f641010-21f7-4ea1-b72d-790e8b1c41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: Certainly! Here is the extracted information from the image:\n",
      "\n",
      "---\n",
      "\n",
      "**PIF VISION REALIZATION PROGRAM**\n",
      "\n",
      "**Strategic Review | PIF Vision Realization Program**\n",
      "\n",
      "**The Public Investment Fund's commitment in Saudi Vision 2030: Working towards driving sustainable and transformative economic change through the development of the Saudi economy and building its international asset portfolio, with the aim of achieving long-term sustainable returns and fostering economic diversification.**\n",
      "\n",
      "**In 2021, PIF launched its Vision Realization Program 2021-2025, redefining a critical step in the Fund’s evolution and the alignment of its strategic objectives with its resources and ambitions. The VIP emphasizes PIF’s role in shaping Saudi Arabia’s vision for a diversified and sustainable future, positioning the Fund as a practical player in Saudi Arabia’s broader economic narrative.**\n",
      "\n",
      "**EXPECTED IMPACT BY 2025**\n",
      "\n",
      "- **Cumulative Non-of GDP Contribution:** SAR 1.2 TN (cumulative)\n",
      "- **Job Creation:** 1.8 MN (Direct, Indirect and Induced jobs)\n",
      "- **Contribution to Local Content:** 60% (including PIF and its portfolio companies)\n",
      "- **Cumulative Non-governmental Investments:** SAR 1.2 TN (includes domestic and foreign direct investment)\n",
      "- **13 Strategic Sectors of Focus:**\n",
      "  - Aerospace and Defense\n",
      "  - Real Estate\n",
      "  - Metals and Mining\n",
      "  - Consumer Goods and Retail\n",
      "  - Automotive\n",
      "  - Entertainment, Leisure and Sports\n",
      "  - Utilities and Renewables\n",
      "  - Financial Services\n",
      "  - Building Constructions, Materials and Services\n",
      "  - Transport and Logistics\n",
      "  - Health Care\n",
      "  - Food and Agriculture\n",
      "  - Telecom, Media and Technology\n",
      "\n",
      "**SOURCE OF FUNDING**\n",
      "- Capital injections from the government\n",
      "- Government assets transferred to PIF\n",
      "- Proceeds from IPOs\n",
      "- Retained earnings from investments\n",
      "\n",
      "**DIRECT OBJECTIVES**\n",
      "- Cover the assets of PIF\n",
      "- Build strategic economic partnerships through PIF\n",
      "- Locate cutting-edge technology and knowledge through PIF\n",
      "\n",
      "**STRATEGIC PILLARS**\n",
      "- Launch and grow domestic sectors\n",
      "- Develop domestic real estate projects\n",
      "- Develop mega-projects\n",
      "- Capitalize on PIF’s equity assets internationally\n",
      "- Support economic development and enable Vision 2030\n",
      "- Exploit portfolio synergies and create value for shareholders\n",
      "- Ensure funding and strengthen PIF’s balance sheet\n",
      "- Strengthen PIF as an institution\n",
      "\n",
      "**2025 TARGETS**\n",
      "- **Assets Under Management (AUM):** SAR 4 TN (Baseline: SAR 2.9 TN in 2020)\n",
      "- **Share of PIF Assets in New Sectors:** 21% (Baseline: 10% in 2020)\n",
      "- **New Local Investments:** SAR 150 BN (Annual Minimum)\n",
      "- **Share of PIF Assets in International Sectors:** 24% (Baseline: 10% in 2020)\n",
      "\n",
      "---\n",
      "\n",
      "**Page Numbers:**\n",
      "- Page 26\n",
      "- Page 27\n",
      "\n",
      "---\n",
      "Execution time: 15.14 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./pif.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. page numbers also\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fba3a-d75d-493b-974d-84c61bc7c97c",
   "metadata": {},
   "source": [
    "# Arabic images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5692bf-87a2-4f51-84b2-747ae827bde0",
   "metadata": {},
   "source": [
    "# well lets try it out what do you think ?\n",
    "(N/G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc04ba6-9c5d-46aa-bab5-4ebb621a89aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: The image appears to be a pie chart or a circular diagram with various segments, each labeled with text and numerical values. Here is the detailed breakdown:\n",
      "\n",
      "1. **Title**: \n",
      "   - The title at the top reads: \"ةيفرصملا تاروطتلا مهأ\" which translates to \"The Impact of Financial Policies on Economic Growth.\"\n",
      "\n",
      "2. **Segments**:\n",
      "   - **Segment 1**: \n",
      "     - Label: \"هينج رايلم\" (Economic Growth)\n",
      "     - Value: 4798,9%\n",
      "   - **Segment 2**: \n",
      "     - Label: \"هينج رايلم\" (Economic Growth)\n",
      "     - Value: 9450,8%\n",
      "   - **Segment 3**: \n",
      "     - Label: \"ضورفلا لامجإ\" (Public Sector Performance)\n",
      "     - Value: 50,8%\n",
      "   - **Segment 4**: \n",
      "     - Label: \"عئادولا لامجإ\" (Private Sector Performance)\n",
      "     - Value: 50,8%\n",
      "   - **Segment 5**: \n",
      "     - Label: \"ضورفلا ةظفحم لامجإ\" (Public Sector Performance)\n",
      "     - Value: 17,7%\n",
      "   - **Segment 6**: \n",
      "     - Label: \"ضورفلا ةظفحم لامجإ\" (Public Sector Performance)\n",
      "     - Value: 1,2%\n",
      "   - **Segment 7**: \n",
      "     - Label: \"ضورفلا ةظفحم لامجإ\" (Public Sector Performance)\n",
      "     - Value: 2,0%\n",
      "\n",
      "3. **Additional Information**:\n",
      "   - There is a section at the bottom that seems to provide additional context or calculations.\n",
      "   - It mentions percentages related to \"تامصصخم ةبسن\" (Economic Stability) and \"ضورفلا تلاهمسلاو ضورفلا ةظفحملا ريغ\" (Public Sector Performance).\n",
      "\n",
      "4. **Overall Context**:\n",
      "   - The diagram seems to analyze the impact of financial policies on economic growth by comparing public and private sector performances.\n",
      "\n",
      "This is a detailed interpretation of the image content. If you need further clarification or have specific questions about the data, feel free to ask!\n",
      "Execution time: 11.60 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./arabic_cbe.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. page numbers also\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a622b2-ab2d-4127-8f00-a206d146aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: فيما يلي تفاصيل الصورة المقدمة:\n",
      "\n",
      "- **العنوان الرئيسي**: ةيفرصملا تاروطتلا مهأ\n",
      "\n",
      "- **البيانات الرئيسية**:\n",
      "  - **4798,9**: هينج رايلم\n",
      "  - **9450,8**: هينج رايلم\n",
      "  - **٪50,8**: عئادولا لىإ ضورقلا ةبسن\n",
      "  - **٪17,7**: ىلع دئاعلا لدعم ةيكمللا قوقح طسوتم فيرصملا زاهجلل\n",
      "  - **٪1,2**: ىلع دئاعلا لدعم لوصلأا طسوتم\n",
      "  - **٪2,3**: ضورقلا ةظفحم لامج ةيامحلإا تلايهستلاو تلايهستلاو ضورقلا لىإ ةقطنملا ريغ ضورقلا لامجو تلايهستلاو\n",
      "  - **٪91,6**: تامصصخم ةبسن لىإ تلايهستلاو ضورقلا تلايهستلاو ضورقلا ةقطنملا ريغ\n",
      "\n",
      "- **البيانات الإجمالية**: عئادولا لامجإ و ضورقلا لامجإ.\n",
      "Execution time: 7.39 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./arabic_cbe.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert Arabic OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. page numbers also .. Generate in arabic text only\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6167083-5bdc-4b31-bbc8-f37755e85fab",
   "metadata": {},
   "source": [
    "# lets expand the res of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efc4651-230f-4641-b54e-968d069023d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1fc5eecb7140ed95d95bfdabc5bb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from typing import List, Union, Dict\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class QwenVLProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "        min_pixels: int = 128*25*25,\n",
    "        max_pixels: int = 1500*35*35,\n",
    "        cache_dir: str = None  \n",
    "\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the QwenVL processor with custom configuration.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name or path of the model to load\n",
    "            device: Device to run the model on ('cuda' or 'cpu')\n",
    "            use_flash_attention: Whether to use flash attention\n",
    "            min_pixels: Minimum number of pixels for image processing\n",
    "            max_pixels: Maximum number of pixels for image processing\n",
    "        \"\"\"\n",
    "        # Configure CUDA memory allocation\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Load model and assign to self\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\",  # Changed from use_flash_attention_2\n",
    "            use_cache=True,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # Load processor and assign to self\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            min_pixels=min_pixels,\n",
    "            max_pixels=max_pixels,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def _encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode a local image file to base64.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the local image file\n",
    "\n",
    "        Returns:\n",
    "            Base64 encoded string of the image\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def prepare_messages(\n",
    "        self,\n",
    "        image_paths: Union[str, List[str]],\n",
    "        prompt: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare messages for the model using local image paths.\n",
    "\n",
    "        Args:\n",
    "            image_paths: Single path or list of paths to local images\n",
    "            prompt: Text prompt to process with the images\n",
    "\n",
    "        Returns:\n",
    "            List of formatted messages for the model\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "\n",
    "        messages = []\n",
    "        for path in image_paths:\n",
    "            encoded_image = self._encode_image(path)\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": encoded_image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    def process_images(\n",
    "        self,\n",
    "        image_paths: Union[str, List[str]],\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 2000,\n",
    "        temperature: float = 0.1,\n",
    "        top_p: float = 0.9\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process local images with the given prompt.\n",
    "\n",
    "        Args:\n",
    "            image_paths: Single path or list of paths to local images\n",
    "            prompt: Text prompt to process with the images\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            top_p: Top-p sampling parameter\n",
    "\n",
    "        Returns:\n",
    "            List of generated responses for each image\n",
    "        \"\"\"\n",
    "        messages = self.prepare_messages(image_paths, prompt)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = self.processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            inputs = inputs.to(self.device)\n",
    "\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):]\n",
    "                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            output_text = self.processor.batch_decode(\n",
    "                generated_ids_trimmed,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "\n",
    "        return output_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    processor = QwenVLProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efc80b8-e0c1-4b25-ba1f-9568323ce76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: الصورة تقدم معلومات عن أهم التطورات المصرفية في مصر، وتشمل البيانات التالية:\n",
      "\n",
      "1. إجمالي القروض: 4798,9 مليار جنيه.\n",
      "2. إجمالي الودائع: 9450,8 مليار جنيه.\n",
      "3. نسبة القروض إلى الودائع: 50,8%.\n",
      "\n",
      "بالإضافة إلى ذلك، هناك بعض الأرقام الأخرى التي تم تقديمها في الصورة:\n",
      "\n",
      "- معدل العائد على متوسط حقوق الملكية للجهاز المصرفي: 17,7%.\n",
      "- معدل العائد على متوسط الأصول: 1,2%.\n",
      "- إجمالي محفظة القروض والتسهيلات غير المنتظمة إلى إجمالي القروض والتسهيلات: 20,3%.\n",
      "- نسبة مخصصات القروض والتسهيلات إلى القروض والتسهيلات غير المنتظمة: 91,6%.\n",
      "Execution time: 6.23 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./arabic_cbe.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert Arabic OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. page numbers also .. Generate in arabic text\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2878a536-9dc5-4e74-b28f-ccae5d8c33a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: Certainly! Here is the detailed information extracted from the image:\n",
      "\n",
      "---\n",
      "\n",
      "### PIF Vision Realization Program\n",
      "\n",
      "#### Strategic Review | PIF Vision Realization Program\n",
      "\n",
      "**PIF Vision Realization Program:**\n",
      "- **The Public Investment Fund (PIF) is a cornerstone in Saudi Vision 2030, tasked with driving sustainable and transformative economic change.**\n",
      "- **Focused on strengthening the local economy and building its international asset portfolio, PIF is dedicated to maximizing sustainable returns and fostering economic diversification.**\n",
      "\n",
      "#### Source of Funding:\n",
      "- Capital injections from the government\n",
      "- Government assets transferred to PIF\n",
      "- Loans and debt instruments\n",
      "- Retained earnings from investments\n",
      "\n",
      "#### Direct Objectives:\n",
      "- Grow the assets of PIF\n",
      "- Unlock new sectors through PIF\n",
      "- Build strategic economic partnerships through PIF\n",
      "- Localize cutting-edge technology and knowledge through PIF\n",
      "\n",
      "#### Strategic Pillars:\n",
      "- Launch and grow domestic sectors\n",
      "- Develop domestic real estate projects\n",
      "- Develop giga-projects\n",
      "- Grow and diversify PIF's assets internationally\n",
      "- Support national development and enable Vision 2030\n",
      "- Exploit portfolio synergies and create strategic and operational value\n",
      "- Diversify funding and strengthen PIF’s balance sheet\n",
      "- Strengthen PIF as an institution\n",
      "\n",
      "#### Expected Impact by 2025:\n",
      "- Cumulative Non-oil GDP Contribution: SAR 1.2 TN (Cumulatively)\n",
      "- Job Creation: 1.8 MN (Direct, indirect and induced jobs)\n",
      "- Contribution to Local Content: 60% (Including PIF and its portfolio companies)\n",
      "- Cumulative Non-governmental Investments: SAR 1.2 TN (Includes domestic and foreign direct investment)\n",
      "\n",
      "#### 2025 Targets:\n",
      "- Assets Under Management (AUM): SAR 4 TN (Baseline: SAR 1.5 Tn in 2020)\n",
      "- New Local Investments: SAR 150 BN (Annual Minimum)\n",
      "- Share of PIF Assets in New Sectors: 21% (Baseline: 1.5% in 2020)\n",
      "- Share of PIF Assets in International Sectors: 24% (Baseline: 30% in 2020)\n",
      "\n",
      "#### 13 Strategic Sectors of Focus:\n",
      "1. Aerospace and Defense\n",
      "2. Automotive\n",
      "3. Entertainment, Leisure and Sports\n",
      "4. Financial Services\n",
      "5. Building Constructions, Materials and Services\n",
      "6. Consumer Goods and Retail\n",
      "7. Health Care\n",
      "8. Food and Agriculture\n",
      "9. Telecom, Media and Technology\n",
      "10. Metals and Mining\n",
      "11. Real Estate\n",
      "12. Transport and Logistics\n",
      "13. Utilities and Renewables\n",
      "\n",
      "---\n",
      "\n",
      "**Page Numbers:**\n",
      "- Page 26\n",
      "- Page 27\n",
      "\n",
      "--- \n",
      "\n",
      "This summary captures all the key points and numerical data presented in the image.\n",
      "Execution time: 14.40 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./pif.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. page numbers also\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0be4ae-d3a1-402f-ab3f-77cf5acf9057",
   "metadata": {},
   "source": [
    "# always good ? no :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc8a5b1-6357-407d-a431-0a8ddf2e1631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image result: الصداقة مواقف وليس عشرون عاماً\n",
      "Execution time: 0.45 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "image_path = \"./friend.png\"\n",
    "result = processor.process_images(\n",
    "    image_path,\n",
    "    prompt=\"\"\"You are an expert Arabic OCR model who can read and interpret hard images in details\n",
    "    and in great precision. Given these images extract every detail of it in an organized format,\n",
    "    include any numbers you see .. Generate in arabic text\"\"\"\n",
    ")\n",
    "\n",
    "end_time = time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Single image result: {result[0]}\")\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2422-caed-422c-a746-8d73e7febab1",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "## can i embedd an image directly ?\n",
    "## can the model see ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084801e-bf13-4c13-81a5-5f7144b3fc74",
   "metadata": {},
   "source": [
    "### 1. VisRag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350c039",
   "metadata": {},
   "source": [
    "### lets try the pdf and convert it to images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbac913",
   "metadata": {},
   "source": [
    "# now use VisRag-Ret to retrieve relevant information from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f31be5-93ec-4712-8363-fe6ddca5d991",
   "metadata": {},
   "source": [
    "### https://huggingface.co/openbmb/VisRAG-Ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e18c23",
   "metadata": {},
   "source": [
    "#### first prepare the pdf and convert to images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806bb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Collecting httplib2\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting configparser\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Collecting nipype\n",
      "  Downloading nipype-1.9.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mmm\n",
      "Collecting pyxnat\n",
      "  Downloading pyxnat-1.6.3-py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting configobj\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from fitz) (2.1.2)\n",
      "Requirement already satisfied: pillow in /venv/main/lib/python3.10/site-packages (from pdf2image) (11.0.0)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting importlib-resources>=5.12\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /venv/main/lib/python3.10/site-packages (from nibabel->fitz) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20 in /venv/main/lib/python3.10/site-packages (from nibabel->fitz) (24.2)\n",
      "Collecting pydot>=1.2.3\n",
      "  Downloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /venv/main/lib/python3.10/site-packages (from nipype->fitz) (3.17.0)\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting traits>=6.2\n",
      "  Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting simplejson>=3.8.0\n",
      "  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting click>=6.6.0\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting acres\n",
      "  Downloading acres-0.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: networkx>=2.5 in /venv/main/lib/python3.10/site-packages (from nipype->fitz) (3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /venv/main/lib/python3.10/site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Collecting looseversion!=1.2\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting prov>=1.5.2\n",
      "  Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting etelemetry>=0.3.1\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Collecting puremagic\n",
      "  Downloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: requests>=2.20 in /venv/main/lib/python3.10/site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Collecting lxml>=4.3\n",
      "  Downloading lxml-5.3.1-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting pathlib>=1.0\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Collecting ci-info>=0.2\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.10/site-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Collecting isodate<0.7.0,>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2.3.0)\n",
      "Installing collected packages: pytz, puremagic, pathlib, looseversion, tzdata, traits, simplejson, scipy, pyparsing, pymupdf, pdf2image, lxml, isodate, importlib-resources, configparser, configobj, click, ci-info, rdflib, pyxnat, pydot, pandas, nibabel, httplib2, etelemetry, acres, prov, nipype, fitz\n",
      "Successfully installed acres-0.3.0 ci-info-0.3.0 click-8.1.8 configobj-5.0.9 configparser-7.2.0 etelemetry-0.3.1 fitz-0.0.1.dev2 httplib2-0.22.0 importlib-resources-6.5.2 isodate-0.6.1 looseversion-1.3.0 lxml-5.3.1 nibabel-5.3.2 nipype-1.9.2 pandas-2.2.3 pathlib-1.0.1 pdf2image-1.17.0 prov-2.0.1 puremagic-1.28 pydot-3.0.4 pymupdf-1.25.3 pyparsing-3.2.1 pytz-2025.1 pyxnat-1.6.3 rdflib-6.3.2 scipy-1.15.2 simplejson-3.20.1 traits-7.0.2 tzdata-2025.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz pymupdf pdf2image \n",
    "# sudo apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1697b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: moodys-rating-report.pdf\n",
      "File size: 132.03 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdf(url, output_filename):\n",
    "    \"\"\"\n",
    "    Download a PDF file from a URL and save it with the specified filename\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the PDF file\n",
    "        output_filename (str): Name to save the file as\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send GET request to the URL\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        # Raise an exception if the request was unsuccessful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if the content type is PDF\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'application/pdf' not in content_type and '.pdf' not in url:\n",
    "            print(f\"Warning: The content might not be a PDF. Content-Type: {content_type}\")\n",
    "        \n",
    "        # Save the file\n",
    "        with open(output_filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        \n",
    "        print(f\"Successfully downloaded: {output_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(output_filename) / 1024:.2f} KB\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        return False\n",
    "\n",
    "# URL of the PDF to download\n",
    "url = \"https://www.pif.gov.sa/-/media/project/pif-corporate/pif-corporate-site/investors/credit-rating/pdf/moodys-rating-report.pdf\"\n",
    "\n",
    "# Output filename\n",
    "output_filename = \"moodys-rating-report.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "download_pdf(url, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1491fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted page 1 to moodys-rating-report/page_1.jpg\n",
      "Converted page 2 to moodys-rating-report/page_2.jpg\n",
      "Converted page 3 to moodys-rating-report/page_3.jpg\n",
      "Converted page 4 to moodys-rating-report/page_4.jpg\n",
      "Converted page 5 to moodys-rating-report/page_5.jpg\n",
      "Converted page 6 to moodys-rating-report/page_6.jpg\n",
      "Converted page 7 to moodys-rating-report/page_7.jpg\n",
      "Converted page 8 to moodys-rating-report/page_8.jpg\n",
      "Converted page 9 to moodys-rating-report/page_9.jpg\n",
      "Converted page 10 to moodys-rating-report/page_10.jpg\n",
      "Converted page 11 to moodys-rating-report/page_11.jpg\n",
      "\n",
      "Successfully converted 11 pages\n",
      "Output files: ['moodys-rating-report/page_1.jpg', 'moodys-rating-report/page_2.jpg', 'moodys-rating-report/page_3.jpg', 'moodys-rating-report/page_4.jpg', 'moodys-rating-report/page_5.jpg', 'moodys-rating-report/page_6.jpg', 'moodys-rating-report/page_7.jpg', 'moodys-rating-report/page_8.jpg', 'moodys-rating-report/page_9.jpg', 'moodys-rating-report/page_10.jpg', 'moodys-rating-report/page_11.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_jpg(pdf_path: str, output_folder: str, dpi: int = 300) -> list:\n",
    "    \"\"\"\n",
    "    Convert PDF pages to JPG images using pdf2image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf_path : str\n",
    "        Path to the input PDF file\n",
    "    output_folder : str\n",
    "        Path to the folder where JPG images will be saved\n",
    "    dpi : int, optional\n",
    "        DPI for rendering (higher means better quality but larger files)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of paths to the generated JPG files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input PDF file\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to list of images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        output_files = []\n",
    "        \n",
    "        # Save each image\n",
    "        for i, image in enumerate(images):\n",
    "            output_path = os.path.join(output_folder, f\"page_{i+1}.jpg\")\n",
    "            image.save(output_path, \"JPEG\")\n",
    "            output_files.append(output_path)\n",
    "            print(f\"Converted page {i+1} to {output_path}\")\n",
    "            \n",
    "        return output_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error converting PDF: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Convert a sample PDF\n",
    "        pdf_file = \"moodys-rating-report.pdf\"\n",
    "        output_dir = \"moodys-rating-report\"\n",
    "        \n",
    "        # Convert PDF to images (higher DPI for better quality)\n",
    "        image_files = convert_pdf_to_jpg(pdf_file, output_dir, dpi=400)\n",
    "        \n",
    "        print(f\"\\nSuccessfully converted {len(image_files)} pages\")\n",
    "        print(\"Output files:\", image_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01304233-5210-4b1b-8291-d9b14ec394f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SentencePiece in /venv/main/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: timm in /venv/main/lib/python3.10/site-packages (1.0.15)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /venv/main/lib/python3.10/site-packages (from timm) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /venv/main/lib/python3.10/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.10/site-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.2.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.10/site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from torchvision->timm) (2.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install SentencePiece timm\n",
    "# restart required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff2ac20-bc39-4031-9d67-824cf4f83a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load images from directory: moodys-rating-report\n",
      "Embeddings directory: embeddings\n",
      "Checking for existing embeddings at: embeddings/embeddings.pkl\n",
      "Checking for paths file at: embeddings/image_paths.json\n",
      "No existing embedding files found\n",
      "\n",
      "Computing new embeddings...\n",
      "Computing embeddings for 11 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee945e6d7ce4bcebb60f6a8e88e0410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecae290eefc4439a06b41124577ee85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.py:   0%|          | 0.00/983 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- tokenizer.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed77d8ee1c204c868b756b371664eca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d4a3b719a4f12a125eabbda81c45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/765 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbd3bcebad7493e956988a058a876a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef2a56358c64751887f6c53ecc1dabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346b34f2181945a394d76bcf969f53fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_minicpm.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- configuration_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3787a596cea41feb435384426c4137f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_visrag_ret.py:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806fa85319174bbc9ebdcde5f9d5ff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "resampler.py:   0%|          | 0.00/5.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- resampler.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f13041377ea4a0c8fb688b3fb5f9b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_minicpm.py:   0%|          | 0.00/71.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- modeling_minicpm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe16f22520a41d18640f601949e1c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_minicpmv.py:   0%|          | 0.00/21.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- modeling_minicpmv.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/openbmb/VisRAG-Ret:\n",
      "- modeling_visrag_ret.py\n",
      "- resampler.py\n",
      "- modeling_minicpm.py\n",
      "- modeling_minicpmv.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c053e671007b4b7999d457eaa9fb0071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8977fa8b3e49188f589cf36c2c0a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aecee3b8a8b4d41a59303532b6c9cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ac65f58414fbe8aaac4c454af67c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c22401ec22c44baa5374e3de727fa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new embeddings to embeddings\n",
      "\n",
      "Query: How does PIF's investment strategy align with Saudi Arabia's Vision 2030 goals?\n",
      "\n",
      "Result 1:\n",
      "Image: page_3.jpg\n",
      "Score: 0.3717\n",
      "\n",
      "Result 2:\n",
      "Image: page_4.jpg\n",
      "Score: 0.3493\n",
      "\n",
      "Result 3:\n",
      "Image: page_2.jpg\n",
      "Score: 0.3273\n",
      "\n",
      "Result 4:\n",
      "Image: page_1.jpg\n",
      "Score: 0.3074\n",
      "\n",
      "Result 5:\n",
      "Image: page_5.jpg\n",
      "Score: 0.2841\n",
      "\n",
      "Result 6:\n",
      "Image: page_6.jpg\n",
      "Score: 0.2780\n",
      "\n",
      "Result 7:\n",
      "Image: page_8.jpg\n",
      "Score: 0.2461\n",
      "\n",
      "Result 8:\n",
      "Image: page_7.jpg\n",
      "Score: 0.1820\n",
      "\n",
      "Result 9:\n",
      "Image: page_9.jpg\n",
      "Score: 0.1339\n",
      "\n",
      "Result 10:\n",
      "Image: page_10.jpg\n",
      "Score: 0.0859\n",
      " \n",
      "time: 88.5887 second\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "class ImageRetriever:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize basic attributes without loading the model.\"\"\"\n",
    "        self.images = []\n",
    "        self.image_paths = []\n",
    "        self.embeddings = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def _init_model(self, model_name=\"openbmb/VisRAG-Ret\", use_cuda=True):\n",
    "        \"\"\"Initialize the model only when needed.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.bfloat16 if device == 'cuda' else torch.float32,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "            self.model.eval()\n",
    "\n",
    "    def weighted_mean_pooling(self, hidden, attention_mask):\n",
    "        \"\"\"Apply weighted mean pooling to the hidden states.\"\"\"\n",
    "        attention_mask_ = attention_mask * attention_mask.cumsum(dim=1)\n",
    "        s = torch.sum(hidden * attention_mask_.unsqueeze(-1).float(), dim=1)\n",
    "        d = attention_mask_.sum(dim=1, keepdim=True).float()\n",
    "        return s / d\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, text_or_image_list):\n",
    "        \"\"\"Encode text queries or images into embeddings.\"\"\"\n",
    "        self._init_model()  # Initialize model only if needed\n",
    "            \n",
    "        if isinstance(text_or_image_list[0], str):\n",
    "            inputs = {\n",
    "                \"text\": text_or_image_list,\n",
    "                'image': [None] * len(text_or_image_list),\n",
    "                'tokenizer': self.tokenizer\n",
    "            }\n",
    "        else:\n",
    "            inputs = {\n",
    "                \"text\": [''] * len(text_or_image_list),\n",
    "                'image': text_or_image_list,\n",
    "                'tokenizer': self.tokenizer\n",
    "            }\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        attention_mask = outputs.attention_mask\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        reps = self.weighted_mean_pooling(hidden, attention_mask)\n",
    "        embeddings = F.normalize(reps, p=2, dim=1).detach().cpu().numpy()\n",
    "        return embeddings\n",
    "    def load_images(self, image_dir, save_dir=None):\n",
    "        \"\"\"Load images and embeddings, computing only if necessary.\"\"\"\n",
    "        print(f\"\\nAttempting to load images from directory: {image_dir}\")\n",
    "        print(f\"Embeddings directory: {save_dir}\")\n",
    "\n",
    "        if not save_dir:\n",
    "            print(\"No save_dir provided, will compute embeddings without saving\")\n",
    "            should_compute = True\n",
    "        else:\n",
    "            # Check for existing embeddings\n",
    "            embeddings_path = os.path.join(save_dir, 'embeddings.pkl')\n",
    "            paths_file = os.path.join(save_dir, 'image_paths.json')\n",
    "            \n",
    "            print(f\"Checking for existing embeddings at: {embeddings_path}\")\n",
    "            print(f\"Checking for paths file at: {paths_file}\")\n",
    "\n",
    "            if os.path.exists(embeddings_path) and os.path.exists(paths_file):\n",
    "                try:\n",
    "                    # Load embeddings and paths\n",
    "                    print(\"Found existing embedding files, attempting to load...\")\n",
    "                    with open(embeddings_path, 'rb') as f:\n",
    "                        self.embeddings = pickle.load(f)\n",
    "                    with open(paths_file, 'r') as f:\n",
    "                        self.image_paths = json.load(f)['image_paths']\n",
    "                    \n",
    "                    # Verify image paths still exist\n",
    "                    missing_images = [p for p in self.image_paths if not os.path.exists(p)]\n",
    "                    if missing_images:\n",
    "                        print(f\"Found {len(missing_images)} missing images, will recompute\")\n",
    "                        should_compute = True\n",
    "                    else:\n",
    "                        # Load images\n",
    "                        print(\"Loading images from saved paths...\")\n",
    "                        self.images = []\n",
    "                        for path in self.image_paths:\n",
    "                            image = Image.open(path).convert('RGB')\n",
    "                            self.images.append(image)\n",
    "                        \n",
    "                        print(f\"Successfully loaded {len(self.images)} images and their embeddings\")\n",
    "                        return\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading saved embeddings: {e}\")\n",
    "                    print(\"Will recompute embeddings\")\n",
    "                    should_compute = True\n",
    "            else:\n",
    "                print(\"No existing embedding files found\")\n",
    "                should_compute = True\n",
    "\n",
    "        # If we get here, we need to compute embeddings\n",
    "        print(\"\\nComputing new embeddings...\")\n",
    "        supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n",
    "        self.images = []\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Load images\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if os.path.splitext(filename)[1].lower() in supported_formats:\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    self.images.append(image)\n",
    "                    self.image_paths.append(image_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filename}: {str(e)}\")\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(f\"No valid images found in {image_dir}\")\n",
    "\n",
    "        # Compute embeddings\n",
    "        print(f\"Computing embeddings for {len(self.images)} images...\")\n",
    "        self.embeddings = self.encode(self.images)\n",
    "        \n",
    "        # Save if requested\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            with open(os.path.join(save_dir, 'embeddings.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "            with open(os.path.join(save_dir, 'image_paths.json'), 'w') as f:\n",
    "                json.dump({'image_paths': self.image_paths}, f)\n",
    "            print(f\"Saved new embeddings to {save_dir}\")\n",
    "\n",
    "    def query(self, question, k=3):\n",
    "        \"\"\"Query the images with a question and return top-k most relevant images.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No images loaded. Please load images first using load_images()\")\n",
    "            \n",
    "        # Prepare and encode query\n",
    "        query = [\"Represent this query for retrieving relevant documents: \" + question]\n",
    "        query_embedding = self.encode(query)\n",
    "        \n",
    "        # Get top-k results\n",
    "        scores = (query_embedding @ self.embeddings.T)[0]\n",
    "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'image_path': self.image_paths[idx],\n",
    "                'score': float(scores[idx]),\n",
    "                'image': self.images[idx]\n",
    "            }\n",
    "            for idx in top_k_indices\n",
    "        ]\n",
    "\n",
    "def main():\n",
    "    # Initialize retriever\n",
    "    start_time = time.time()\n",
    "    retriever = ImageRetriever()\n",
    "    \n",
    "    # Define directories\n",
    "    image_dir = \"moodys-rating-report\"  # Replace with your image directory\n",
    "    embeddings_dir = \"embeddings\"  # Directory to save/load embeddings\n",
    "    \n",
    "    # Load images and compute/load embeddings\n",
    "    retriever.load_images(image_dir, save_dir=embeddings_dir)\n",
    "    \n",
    "    # Example queries\n",
    "    questions = [\n",
    "        \"How does PIF's investment strategy align with Saudi Arabia's Vision 2030 goals?\",\n",
    "    ]\n",
    "    \n",
    "    # Process each query\n",
    "    for question in questions:\n",
    "        print(f\"\\nQuery: {question}\")\n",
    "        results = retriever.query(question, k=10)\n",
    "        \n",
    "        # Print results\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"Image: {os.path.basename(result['image_path'])}\")\n",
    "            print(f\"Score: {result['score']:.4f}\")\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    print(\" \")\n",
    "    print(f\"time: {total_execution_time:.4f} second\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105398c-3037-46cb-806e-6e3c17dcbe28",
   "metadata": {},
   "source": [
    "## check if it matches claude 3.7\n",
    "https://claude.ai/share/cbc8e34d-11d8-40ec-9a09-9b39f634cc0b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075db1f",
   "metadata": {},
   "source": [
    "# now lets try colqwen2.5 Ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7b16e-ea1c-4a95-aee8-5ff11f01a69a",
   "metadata": {},
   "source": [
    "https://github.com/illuin-tech/colpali?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10305c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colpali-engine\n",
      "  Downloading colpali_engine-0.3.8-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 KB\u001b[0m \u001b[31m945.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from colpali-engine) (2.32.3)\n",
      "Collecting gputil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from colpali-engine) (2.1.2)\n",
      "Collecting peft<0.15.0,>=0.14.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: torch>=2.2.0 in /venv/main/lib/python3.10/site-packages (from colpali-engine) (2.6.0+cu124)\n",
      "Collecting transformers<4.48.0,>=4.47.0\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Requirement already satisfied: pillow>=10.0.0 in /venv/main/lib/python3.10/site-packages (from colpali-engine) (11.0.0)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (1.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (0.28.1)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (24.2)\n",
      "Requirement already satisfied: safetensors in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (0.5.3)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali-engine) (6.1.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (0.6.2)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (2025.2.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (3.17.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (9.1.0.70)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (12.4.127)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali-engine) (3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2.0->colpali-engine) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers<4.48.0,>=4.47.0->colpali-engine) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers<4.48.0,>=4.47.0->colpali-engine) (0.21.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->colpali-engine) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->colpali-engine) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->colpali-engine) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->colpali-engine) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->colpali-engine) (2.1.5)\n",
      "Building wheels for collected packages: gputil\n",
      "doneng wheel for gputil (setup.py) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7410 sha256=bb9eeac36c803cf641f6cbd8484624358e82b1f0f200226a2c91f22355c94c06\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil, transformers, peft, colpali-engine\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.0.dev0\n",
      "    Uninstalling transformers-4.50.0.dev0:\n",
      "      Successfully uninstalled transformers-4.50.0.dev0\n",
      "Successfully installed colpali-engine-0.3.8 gputil-1.4.0 peft-0.14.0 transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install colpali-engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9430e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/illuin-tech/colpali\n",
      "  Cloning https://github.com/illuin-tech/colpali to /tmp/pip-req-build-0ilt_7z0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/illuin-tech/colpali /tmp/pip-req-build-0ilt_7z0\n",
      "  Resolved https://github.com/illuin-tech/colpali to commit 71af1932ab57c79dc6ad4b4e2f8e8339754d4bc9\n",
      "  Installing build dependenciesdone\n",
      "done5h  Getting requirements to build wheel ... \u001b[?25l\n",
      "done5h  Preparing metadata (pyproject.toml) ... \u001b[?25l\n",
      "Collecting transformers<4.50.0,>=4.49.0\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Requirement already satisfied: pillow>=10.0.0 in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (11.0.0)\n",
      "Requirement already satisfied: torch>=2.2.0 in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (2.6.0+cu124)\n",
      "Requirement already satisfied: scipy in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (1.15.2)\n",
      "Requirement already satisfied: peft<0.15.0,>=0.14.0 in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (0.14.0)\n",
      "Requirement already satisfied: gputil in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (1.4.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (2.32.3)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.10/site-packages (from colpali_engine==0.3.9.dev35+g71af193) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (1.4.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (6.0.2)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (6.1.1)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from peft<0.15.0,>=0.14.0->colpali_engine==0.3.9.dev35+g71af193) (24.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (9.1.0.70)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (10.3.5.147)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (3.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (0.6.2)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (2025.2.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.127)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (3.17.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers<4.50.0,>=4.49.0->colpali_engine==0.3.9.dev35+g71af193) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.10/site-packages (from transformers<4.50.0,>=4.49.0->colpali_engine==0.3.9.dev35+g71af193) (0.21.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->colpali_engine==0.3.9.dev35+g71af193) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->colpali_engine==0.3.9.dev35+g71af193) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->colpali_engine==0.3.9.dev35+g71af193) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->colpali_engine==0.3.9.dev35+g71af193) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.2.0->colpali_engine==0.3.9.dev35+g71af193) (2.1.5)\n",
      "Building wheels for collected packages: colpali_engine\n",
      "doneilding wheel for colpali_engine (pyproject.toml) ... \u001b[?25l\n",
      "\u001b[?25h  Created wheel for colpali_engine: filename=colpali_engine-0.3.9.dev35+g71af193-py3-none-any.whl size=54073 sha256=87c23204d42a5564d8bbebb053f836345960e1b3dfe7d42698c347993beb1f31\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-etzsgcln/wheels/11/56/6e/b20862306e7f69ee58c838e9bdc087d29ca2ab7492faf5bb0e\n",
      "Successfully built colpali_engine\n",
      "Installing collected packages: transformers, colpali_engine\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "  Attempting uninstall: colpali_engine\n",
      "    Found existing installation: colpali_engine 0.3.8\n",
      "    Uninstalling colpali_engine-0.3.8:\n",
      "      Successfully uninstalled colpali_engine-0.3.8\n",
      "Successfully installed colpali_engine-0.3.9.dev35+g71af193 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/illuin-tech/colpali "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3e0e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: moodys-rating-report.pdf\n",
      "File size: 132.03 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdf(url, output_filename):\n",
    "    \"\"\"\n",
    "    Download a PDF file from a URL and save it with the specified filename\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the PDF file\n",
    "        output_filename (str): Name to save the file as\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send GET request to the URL\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        # Raise an exception if the request was unsuccessful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if the content type is PDF\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'application/pdf' not in content_type and '.pdf' not in url:\n",
    "            print(f\"Warning: The content might not be a PDF. Content-Type: {content_type}\")\n",
    "        \n",
    "        # Save the file\n",
    "        with open(output_filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        \n",
    "        print(f\"Successfully downloaded: {output_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(output_filename) / 1024:.2f} KB\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        return False\n",
    "\n",
    "# URL of the PDF to download\n",
    "url = \"https://www.pif.gov.sa/-/media/project/pif-corporate/pif-corporate-site/investors/credit-rating/pdf/moodys-rating-report.pdf\"\n",
    "\n",
    "# Output filename\n",
    "output_filename = \"moodys-rating-report.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "download_pdf(url, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b592ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted page 1 to moodys-rating-report/page_1.jpg\n",
      "Converted page 2 to moodys-rating-report/page_2.jpg\n",
      "Converted page 3 to moodys-rating-report/page_3.jpg\n",
      "Converted page 4 to moodys-rating-report/page_4.jpg\n",
      "Converted page 5 to moodys-rating-report/page_5.jpg\n",
      "Converted page 6 to moodys-rating-report/page_6.jpg\n",
      "Converted page 7 to moodys-rating-report/page_7.jpg\n",
      "Converted page 8 to moodys-rating-report/page_8.jpg\n",
      "Converted page 9 to moodys-rating-report/page_9.jpg\n",
      "Converted page 10 to moodys-rating-report/page_10.jpg\n",
      "Converted page 11 to moodys-rating-report/page_11.jpg\n",
      "\n",
      "Successfully converted 11 pages\n",
      "Output files: ['moodys-rating-report/page_1.jpg', 'moodys-rating-report/page_2.jpg', 'moodys-rating-report/page_3.jpg', 'moodys-rating-report/page_4.jpg', 'moodys-rating-report/page_5.jpg', 'moodys-rating-report/page_6.jpg', 'moodys-rating-report/page_7.jpg', 'moodys-rating-report/page_8.jpg', 'moodys-rating-report/page_9.jpg', 'moodys-rating-report/page_10.jpg', 'moodys-rating-report/page_11.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_jpg(pdf_path: str, output_folder: str, dpi: int = 300) -> list:\n",
    "    \"\"\"\n",
    "    Convert PDF pages to JPG images using pdf2image.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf_path : str\n",
    "        Path to the input PDF file\n",
    "    output_folder : str\n",
    "        Path to the folder where JPG images will be saved\n",
    "    dpi : int, optional\n",
    "        DPI for rendering (higher means better quality but larger files)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of paths to the generated JPG files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input PDF file\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Convert PDF to list of images\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        output_files = []\n",
    "        \n",
    "        # Save each image\n",
    "        for i, image in enumerate(images):\n",
    "            output_path = os.path.join(output_folder, f\"page_{i+1}.jpg\")\n",
    "            image.save(output_path, \"JPEG\")\n",
    "            output_files.append(output_path)\n",
    "            print(f\"Converted page {i+1} to {output_path}\")\n",
    "            \n",
    "        return output_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error converting PDF: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Convert a sample PDF\n",
    "        pdf_file = \"moodys-rating-report.pdf\"\n",
    "        output_dir = \"moodys-rating-report\"\n",
    "        \n",
    "        # Convert PDF to images (higher DPI for better quality)\n",
    "        image_files = convert_pdf_to_jpg(pdf_file, output_dir, dpi=400)\n",
    "        \n",
    "        print(f\"\\nSuccessfully converted {len(image_files)} pages\")\n",
    "        print(\"Output files:\", image_files)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438208d5-e6a7-4fe4-bf29-28324a3a45b1",
   "metadata": {},
   "source": [
    "## this needs a base model cuz its an Adaptor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5597b-c752-4001-b996-a9f5cd251722",
   "metadata": {},
   "source": [
    "An adapter refers to a lightweight and efficient fine-tuning technique that adds small trainable components to a pre-trained model without modifying the original model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edf399-7d5f-4e9a-92b2-868f7bdc2139",
   "metadata": {},
   "source": [
    "### lets talk about Adaptors for a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df1a060-c6f4-4023-8474-8a206fd204ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.49.0\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Requirement already satisfied: peft==0.14.0 in /venv/main/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (3.17.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.1/468.1 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (6.0.2)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (24.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers==4.49.0) (2.1.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /venv/main/lib/python3.10/site-packages (from peft==0.14.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from peft==0.14.0) (6.1.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /venv/main/lib/python3.10/site-packages (from peft==0.14.0) (1.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (3.3)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.2.0 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (3.2.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (2.21.5)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /venv/main/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.14.0) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.14.0) (1.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.49.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers==4.49.0) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.14.0) (2.1.5)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "Successfully installed huggingface-hub-0.29.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.49.0 peft==0.14.0  #-> restart kernel\n",
    "# !pip install transformers==4.34.0 \n",
    "# !pip uninstall transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ee27fa",
   "metadata": {},
   "source": [
    "https://www.pif.gov.sa/-/media/project/pif-corporate/pif-corporate-site/our-financials/annual-reports/pdf/pif-2023-annual-report-ar.pdf saudi pif ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bdd110-6769-4180-ac4b-7f4e0bb2bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pif_ar.pdf: 100%|██████████| 19.4M/19.4M [00:22<00:00, 862kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: pif_ar.pdf\n",
      "File size: 18908.05 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_pdf(url, output_filename):\n",
    "    \"\"\"\n",
    "    Download a PDF file from a URL and save it with the specified filename\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the PDF file\n",
    "        output_filename (str): Name to save the file as\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send GET request to the URL\n",
    "        response = requests.get(url, stream=True)\n",
    "        \n",
    "        # Raise an exception if the request was unsuccessful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check if the content type is PDF\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'application/pdf' not in content_type and '.pdf' not in url:\n",
    "            print(f\"Warning: The content might not be a PDF. Content-Type: {content_type}\")\n",
    "        \n",
    "        # Get the total file size if available\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        # Initialize the progress bar\n",
    "        progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=output_filename)\n",
    "        \n",
    "        # Save the file\n",
    "        with open(output_filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))\n",
    "        \n",
    "        # Close the progress bar\n",
    "        progress_bar.close()\n",
    "        \n",
    "        print(f\"Successfully downloaded: {output_filename}\")\n",
    "        print(f\"File size: {os.path.getsize(output_filename) / 1024:.2f} KB\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        return False\n",
    "\n",
    "# URL of the PDF to download\n",
    "url = \"https://www.pif.gov.sa/-/media/project/pif-corporate/pif-corporate-site/our-financials/annual-reports/pdf/pif-2023-annual-report-ar.pdf\"\n",
    "# Output filename\n",
    "output_filename = \"pif_ar.pdf\"\n",
    "# Download the PDF\n",
    "download_pdf(url, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eeb2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 80 pages. Starting conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PDF pages: 100%|██████████| 16/16 [00:51<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully converted 80 pages\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_pdf_to_jpg(pdf_path: str, output_folder: str, dpi: int = 400, \n",
    "                       threads: int = 4, batch_size: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Convert PDF pages to JPG images using pdf2image with parallel processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pdf_path : str\n",
    "        Path to the input PDF file\n",
    "    output_folder : str\n",
    "        Path to the folder where JPG images will be saved\n",
    "    dpi : int, optional\n",
    "        DPI for rendering (higher means better quality but larger files)\n",
    "    threads : int, optional\n",
    "        Number of worker threads to use for parallel processing\n",
    "    batch_size : int, optional\n",
    "        Number of pages to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of paths to the generated JPG files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input PDF file\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Get total number of pages first\n",
    "        info = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)\n",
    "        total_pages = convert_from_path(pdf_path, dpi=72, first_page=1, last_page=None, thread_count=1)\n",
    "        num_pages = len(total_pages)\n",
    "        print(f\"PDF has {num_pages} pages. Starting conversion...\")\n",
    "        \n",
    "        output_files = []\n",
    "        \n",
    "        # Define a function to convert a batch of pages\n",
    "        def convert_batch(batch):\n",
    "            start_page, end_page = batch\n",
    "            batch_images = convert_from_path(\n",
    "                pdf_path,\n",
    "                dpi=dpi,\n",
    "                first_page=start_page,\n",
    "                last_page=end_page,\n",
    "                thread_count=1  # Use 1 thread per worker as we're already parallelizing\n",
    "            )\n",
    "            \n",
    "            batch_output_files = []\n",
    "            for i, image in enumerate(batch_images):\n",
    "                page_num = start_page + i\n",
    "                output_path = os.path.join(output_folder, f\"page_{page_num}.jpg\")\n",
    "                image.save(output_path, \"JPEG\")\n",
    "                batch_output_files.append(output_path)\n",
    "            \n",
    "            return batch_output_files\n",
    "        \n",
    "        # Create batches\n",
    "        batches = []\n",
    "        for i in range(1, num_pages + 1, batch_size):\n",
    "            batches.append((i, min(i + batch_size - 1, num_pages)))\n",
    "        \n",
    "        # Process batches in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "            # Submit all batches to the executor\n",
    "            future_to_batch = {executor.submit(convert_batch, batch): batch for batch in batches}\n",
    "            \n",
    "            # Process results as they complete with progress bar\n",
    "            with tqdm(total=len(batches), desc=\"Converting PDF pages\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                    batch_files = future.result()\n",
    "                    output_files.extend(batch_files)\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # Sort output files by page number\n",
    "        output_files.sort(key=lambda x: int(os.path.basename(x).split('_')[1].split('.')[0]))\n",
    "        \n",
    "        return output_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error converting PDF: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Convert a sample PDF\n",
    "        pdf_file = \"pif_ar.pdf\"\n",
    "        output_dir = \"pif_ar\"\n",
    "        \n",
    "        # Convert PDF to images with parallel processing\n",
    "        image_files = convert_pdf_to_jpg(\n",
    "            pdf_file, \n",
    "            output_dir, \n",
    "            dpi=400,\n",
    "            threads=os.cpu_count(),  # Use all available CPU cores\n",
    "            batch_size=5            # Process 5 pages at a time\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nSuccessfully converted {len(image_files)} pages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583649f-74c6-46a6-b0d4-f7afbfc21ec4",
   "metadata": {},
   "source": [
    "### VisRag-Ret with arabic will it perform well ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7cee107-d899-4864-9535-5960d03715c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load images from directory: pif_ar\n",
      "Embeddings directory: embeddings\n",
      "Checking for existing embeddings at: embeddings/embeddings.pkl\n",
      "Checking for paths file at: embeddings/image_paths.json\n",
      "Found existing embedding files, attempting to load...\n",
      "Loading images from saved paths...\n",
      "Successfully loaded 11 images and their embeddings\n",
      "\n",
      "Query: ما هو إجمالي الأصول المدارة لصندوق الاستثمارات العامة حتى عام 2023؟\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa5b85e149a4c6a8ab7bf307daa39b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Image: page_1.jpg\n",
      "Score: 0.2109\n",
      "\n",
      "Result 2:\n",
      "Image: page_4.jpg\n",
      "Score: 0.2034\n",
      "\n",
      "Result 3:\n",
      "Image: page_3.jpg\n",
      "Score: 0.1851\n",
      "\n",
      "Result 4:\n",
      "Image: page_8.jpg\n",
      "Score: 0.1705\n",
      "\n",
      "Result 5:\n",
      "Image: page_2.jpg\n",
      "Score: 0.1602\n",
      "\n",
      "Result 6:\n",
      "Image: page_6.jpg\n",
      "Score: 0.1592\n",
      "\n",
      "Result 7:\n",
      "Image: page_5.jpg\n",
      "Score: 0.1428\n",
      "\n",
      "Result 8:\n",
      "Image: page_10.jpg\n",
      "Score: 0.1279\n",
      "\n",
      "Result 9:\n",
      "Image: page_7.jpg\n",
      "Score: 0.1016\n",
      "\n",
      "Result 10:\n",
      "Image: page_9.jpg\n",
      "Score: 0.1011\n",
      "\n",
      "Query: ما هو إجمالي عائد المساهمين لصندوق الاستثمارات العامة منذ بداية برنامج تحقيق الرؤية؟\n",
      "\n",
      "Result 1:\n",
      "Image: page_1.jpg\n",
      "Score: 0.2240\n",
      "\n",
      "Result 2:\n",
      "Image: page_4.jpg\n",
      "Score: 0.2166\n",
      "\n",
      "Result 3:\n",
      "Image: page_3.jpg\n",
      "Score: 0.2145\n",
      "\n",
      "Result 4:\n",
      "Image: page_2.jpg\n",
      "Score: 0.1876\n",
      "\n",
      "Result 5:\n",
      "Image: page_5.jpg\n",
      "Score: 0.1836\n",
      "\n",
      "Result 6:\n",
      "Image: page_8.jpg\n",
      "Score: 0.1806\n",
      "\n",
      "Result 7:\n",
      "Image: page_6.jpg\n",
      "Score: 0.1601\n",
      "\n",
      "Result 8:\n",
      "Image: page_10.jpg\n",
      "Score: 0.1134\n",
      "\n",
      "Result 9:\n",
      "Image: page_9.jpg\n",
      "Score: 0.1087\n",
      "\n",
      "Result 10:\n",
      "Image: page_7.jpg\n",
      "Score: 0.1013\n",
      " \n",
      "time: 13.8202 second\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "class ImageRetriever:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize basic attributes without loading the model.\"\"\"\n",
    "        self.images = []\n",
    "        self.image_paths = []\n",
    "        self.embeddings = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def _init_model(self, model_name=\"openbmb/VisRAG-Ret\", use_cuda=True):\n",
    "        \"\"\"Initialize the model only when needed.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "            device = 'cuda' if use_cuda and torch.cuda.is_available() else 'cpu'\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.bfloat16 if device == 'cuda' else torch.float32,\n",
    "                trust_remote_code=True\n",
    "            ).to(device)\n",
    "            self.model.eval()\n",
    "\n",
    "    def weighted_mean_pooling(self, hidden, attention_mask):\n",
    "        \"\"\"Apply weighted mean pooling to the hidden states.\"\"\"\n",
    "        attention_mask_ = attention_mask * attention_mask.cumsum(dim=1)\n",
    "        s = torch.sum(hidden * attention_mask_.unsqueeze(-1).float(), dim=1)\n",
    "        d = attention_mask_.sum(dim=1, keepdim=True).float()\n",
    "        return s / d\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, text_or_image_list):\n",
    "        \"\"\"Encode text queries or images into embeddings.\"\"\"\n",
    "        self._init_model()  # Initialize model only if needed\n",
    "            \n",
    "        if isinstance(text_or_image_list[0], str):\n",
    "            inputs = {\n",
    "                \"text\": text_or_image_list,\n",
    "                'image': [None] * len(text_or_image_list),\n",
    "                'tokenizer': self.tokenizer\n",
    "            }\n",
    "        else:\n",
    "            inputs = {\n",
    "                \"text\": [''] * len(text_or_image_list),\n",
    "                'image': text_or_image_list,\n",
    "                'tokenizer': self.tokenizer\n",
    "            }\n",
    "        \n",
    "        outputs = self.model(**inputs)\n",
    "        attention_mask = outputs.attention_mask\n",
    "        hidden = outputs.last_hidden_state\n",
    "        \n",
    "        reps = self.weighted_mean_pooling(hidden, attention_mask)\n",
    "        embeddings = F.normalize(reps, p=2, dim=1).detach().cpu().numpy()\n",
    "        return embeddings\n",
    "    def load_images(self, image_dir, save_dir=None):\n",
    "        \"\"\"Load images and embeddings, computing only if necessary.\"\"\"\n",
    "        print(f\"\\nAttempting to load images from directory: {image_dir}\")\n",
    "        print(f\"Embeddings directory: {save_dir}\")\n",
    "\n",
    "        if not save_dir:\n",
    "            print(\"No save_dir provided, will compute embeddings without saving\")\n",
    "            should_compute = True\n",
    "        else:\n",
    "            # Check for existing embeddings\n",
    "            embeddings_path = os.path.join(save_dir, 'embeddings.pkl')\n",
    "            paths_file = os.path.join(save_dir, 'image_paths.json')\n",
    "            \n",
    "            print(f\"Checking for existing embeddings at: {embeddings_path}\")\n",
    "            print(f\"Checking for paths file at: {paths_file}\")\n",
    "\n",
    "            if os.path.exists(embeddings_path) and os.path.exists(paths_file):\n",
    "                try:\n",
    "                    # Load embeddings and paths\n",
    "                    print(\"Found existing embedding files, attempting to load...\")\n",
    "                    with open(embeddings_path, 'rb') as f:\n",
    "                        self.embeddings = pickle.load(f)\n",
    "                    with open(paths_file, 'r') as f:\n",
    "                        self.image_paths = json.load(f)['image_paths']\n",
    "                    \n",
    "                    # Verify image paths still exist\n",
    "                    missing_images = [p for p in self.image_paths if not os.path.exists(p)]\n",
    "                    if missing_images:\n",
    "                        print(f\"Found {len(missing_images)} missing images, will recompute\")\n",
    "                        should_compute = True\n",
    "                    else:\n",
    "                        # Load images\n",
    "                        print(\"Loading images from saved paths...\")\n",
    "                        self.images = []\n",
    "                        for path in self.image_paths:\n",
    "                            image = Image.open(path).convert('RGB')\n",
    "                            self.images.append(image)\n",
    "                        \n",
    "                        print(f\"Successfully loaded {len(self.images)} images and their embeddings\")\n",
    "                        return\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading saved embeddings: {e}\")\n",
    "                    print(\"Will recompute embeddings\")\n",
    "                    should_compute = True\n",
    "            else:\n",
    "                print(\"No existing embedding files found\")\n",
    "                should_compute = True\n",
    "\n",
    "        # If we get here, we need to compute embeddings\n",
    "        print(\"\\nComputing new embeddings...\")\n",
    "        supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n",
    "        self.images = []\n",
    "        self.image_paths = []\n",
    "\n",
    "        # Load images\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if os.path.splitext(filename)[1].lower() in supported_formats:\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    self.images.append(image)\n",
    "                    self.image_paths.append(image_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filename}: {str(e)}\")\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(f\"No valid images found in {image_dir}\")\n",
    "\n",
    "        # Compute embeddings\n",
    "        print(f\"Computing embeddings for {len(self.images)} images...\")\n",
    "        self.embeddings = self.encode(self.images)\n",
    "        \n",
    "        # Save if requested\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            with open(os.path.join(save_dir, 'embeddings.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.embeddings, f)\n",
    "            with open(os.path.join(save_dir, 'image_paths.json'), 'w') as f:\n",
    "                json.dump({'image_paths': self.image_paths}, f)\n",
    "            print(f\"Saved new embeddings to {save_dir}\")\n",
    "\n",
    "    def query(self, question, k=3):\n",
    "        \"\"\"Query the images with a question and return top-k most relevant images.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"No images loaded. Please load images first using load_images()\")\n",
    "            \n",
    "        # Prepare and encode query\n",
    "        query = [\"Represent this query for retrieving relevant documents: \" + question]\n",
    "        query_embedding = self.encode(query)\n",
    "        \n",
    "        # Get top-k results\n",
    "        scores = (query_embedding @ self.embeddings.T)[0]\n",
    "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'image_path': self.image_paths[idx],\n",
    "                'score': float(scores[idx]),\n",
    "                'image': self.images[idx]\n",
    "            }\n",
    "            for idx in top_k_indices\n",
    "        ]\n",
    "\n",
    "def main():\n",
    "    # Initialize retriever\n",
    "    start_time = time.time()\n",
    "    retriever = ImageRetriever()\n",
    "    \n",
    "    # Define directories\n",
    "    image_dir = \"pif_ar\"  # Replace with your image directory\n",
    "    embeddings_dir = \"embeddings\"  # Directory to save/load embeddings\n",
    "    \n",
    "    # Load images and compute/load embeddings\n",
    "    retriever.load_images(image_dir, save_dir=embeddings_dir)\n",
    "    \n",
    "    # Example queries\n",
    "    questions = [\n",
    "    \"ما هو إجمالي الأصول المدارة لصندوق الاستثمارات العامة حتى عام 2023؟\",\n",
    "    \"ما هو إجمالي عائد المساهمين لصندوق الاستثمارات العامة منذ بداية برنامج تحقيق الرؤية؟\",\n",
    "    ]\n",
    "    \n",
    "    # Process each query\n",
    "    for question in questions:\n",
    "        print(f\"\\nQuery: {question}\")\n",
    "        results = retriever.query(question, k=10)\n",
    "        \n",
    "        # Print results\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"Image: {os.path.basename(result['image_path'])}\")\n",
    "            print(f\"Score: {result['score']:.4f}\")\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    print(\" \")\n",
    "    print(f\"time: {total_execution_time:.4f} second\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ceb7e-6e0b-4a2e-8593-4a24b56096d6",
   "metadata": {},
   "source": [
    "# now lets try colqwen 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167daba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from vidore/colqwen2-base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ca1e490b114e10bf6591baba661a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adapter from vidore/colqwen2-v0.1...\n",
      "Adapter loaded successfully\n",
      "Model and processor loaded successfully\n",
      "Loading images from pif_ar...\n",
      "Loaded image: page_76.jpg\n",
      "Loaded image: page_77.jpg\n",
      "Loaded image: page_78.jpg\n",
      "Loaded image: page_79.jpg\n",
      "Loaded image: page_80.jpg\n",
      "Loaded image: page_56.jpg\n",
      "Loaded image: page_57.jpg\n",
      "Loaded image: page_58.jpg\n",
      "Loaded image: page_59.jpg\n",
      "Loaded image: page_60.jpg\n",
      "Loaded image: page_1.jpg\n",
      "Loaded image: page_2.jpg\n",
      "Loaded image: page_41.jpg\n",
      "Loaded image: page_3.jpg\n",
      "Loaded image: page_42.jpg\n",
      "Loaded image: page_4.jpg\n",
      "Loaded image: page_43.jpg\n",
      "Loaded image: page_5.jpg\n",
      "Loaded image: page_44.jpg\n",
      "Loaded image: page_71.jpg\n",
      "Loaded image: page_45.jpg\n",
      "Loaded image: page_72.jpg\n",
      "Loaded image: page_73.jpg\n",
      "Loaded image: page_74.jpg\n",
      "Loaded image: page_75.jpg\n",
      "Loaded image: page_21.jpg\n",
      "Loaded image: page_66.jpg\n",
      "Loaded image: page_22.jpg\n",
      "Loaded image: page_67.jpg\n",
      "Loaded image: page_23.jpg\n",
      "Loaded image: page_68.jpg\n",
      "Loaded image: page_24.jpg\n",
      "Loaded image: page_69.jpg\n",
      "Loaded image: page_25.jpg\n",
      "Loaded image: page_70.jpg\n",
      "Loaded image: page_51.jpg\n",
      "Loaded image: page_52.jpg\n",
      "Loaded image: page_61.jpg\n",
      "Loaded image: page_53.jpg\n",
      "Loaded image: page_62.jpg\n",
      "Loaded image: page_54.jpg\n",
      "Loaded image: page_26.jpg\n",
      "Loaded image: page_55.jpg\n",
      "Loaded image: page_11.jpg\n",
      "Loaded image: page_31.jpg\n",
      "Loaded image: page_63.jpg\n",
      "Loaded image: page_6.jpg\n",
      "Loaded image: page_32.jpg\n",
      "Loaded image: page_12.jpg\n",
      "Loaded image: page_27.jpg\n",
      "Loaded image: page_7.jpg\n",
      "Loaded image: page_64.jpg\n",
      "Loaded image: page_28.jpg\n",
      "Loaded image: page_65.jpg\n",
      "Loaded image: page_8.jpg\n",
      "Loaded image: page_13.jpg\n",
      "Loaded image: page_33.jpg\n",
      "Loaded image: page_29.jpg\n",
      "Loaded image: page_14.jpg\n",
      "Loaded image: page_34.jpg\n",
      "Loaded image: page_9.jpg\n",
      "Loaded image: page_30.jpg\n",
      "Loaded image: page_15.jpg\n",
      "Loaded image: page_35.jpg\n",
      "Loaded image: page_10.jpg\n",
      "Loaded image: page_16.jpg\n",
      "Loaded image: page_17.jpg\n",
      "Loaded image: page_18.jpg\n",
      "Loaded image: page_19.jpg\n",
      "Loaded image: page_20.jpg\n",
      "Loaded image: page_46.jpg\n",
      "Loaded image: page_47.jpg\n",
      "Loaded image: page_48.jpg\n",
      "Loaded image: page_49.jpg\n",
      "Loaded image: page_50.jpg\n",
      "Loaded image: page_36.jpg\n",
      "Loaded image: page_37.jpg\n",
      "Loaded image: page_38.jpg\n",
      "Loaded image: page_39.jpg\n",
      "Loaded image: page_40.jpg\n",
      "Processing 80 images with 2 queries...\n",
      "Processing queries...\n",
      "Processing images...\n",
      "Calculating similarity scores...\n",
      "\n",
      "Query: ما هو إجمالي الأصول المدارة لصندوق الاستثمارات العامة حتى عام 2023؟\n",
      "  page_1.jpg: 18.5000\n",
      "  page_23.jpg: 16.0000\n",
      "  page_22.jpg: 15.6250\n",
      "  page_75.jpg: 15.5625\n",
      "  page_25.jpg: 15.5625\n",
      "  page_79.jpg: 15.4375\n",
      "  page_11.jpg: 15.3125\n",
      "  page_14.jpg: 15.1875\n",
      "  page_30.jpg: 15.1875\n",
      "  page_48.jpg: 15.1875\n",
      "  page_5.jpg: 15.1250\n",
      "  page_27.jpg: 15.1250\n",
      "  page_34.jpg: 15.1250\n",
      "  page_46.jpg: 15.1250\n",
      "  page_36.jpg: 15.1250\n",
      "  page_58.jpg: 15.0625\n",
      "  page_24.jpg: 15.0625\n",
      "  page_35.jpg: 15.0625\n",
      "  page_26.jpg: 15.0000\n",
      "  page_78.jpg: 14.9375\n",
      "  page_41.jpg: 14.9375\n",
      "  page_73.jpg: 14.9375\n",
      "  page_29.jpg: 14.9375\n",
      "  page_20.jpg: 14.9375\n",
      "  page_50.jpg: 14.9375\n",
      "  page_3.jpg: 14.8750\n",
      "  page_45.jpg: 14.8750\n",
      "  page_42.jpg: 14.8125\n",
      "  page_9.jpg: 14.8125\n",
      "  page_21.jpg: 14.7500\n",
      "  page_31.jpg: 14.7500\n",
      "  page_71.jpg: 14.6875\n",
      "  page_12.jpg: 14.6875\n",
      "  page_64.jpg: 14.6875\n",
      "  page_47.jpg: 14.6875\n",
      "  page_32.jpg: 14.6250\n",
      "  page_28.jpg: 14.6250\n",
      "  page_74.jpg: 14.5625\n",
      "  page_62.jpg: 14.5625\n",
      "  page_68.jpg: 14.5000\n",
      "  page_37.jpg: 14.5000\n",
      "  page_39.jpg: 14.5000\n",
      "  page_72.jpg: 14.4375\n",
      "  page_19.jpg: 14.4375\n",
      "  page_38.jpg: 14.3750\n",
      "  page_57.jpg: 14.3125\n",
      "  page_54.jpg: 14.3125\n",
      "  page_63.jpg: 14.2500\n",
      "  page_10.jpg: 14.2500\n",
      "  page_17.jpg: 14.1875\n",
      "  page_60.jpg: 14.1250\n",
      "  page_70.jpg: 14.1250\n",
      "  page_65.jpg: 14.1250\n",
      "  page_59.jpg: 14.0625\n",
      "  page_67.jpg: 14.0625\n",
      "  page_52.jpg: 14.0625\n",
      "  page_33.jpg: 14.0625\n",
      "  page_66.jpg: 14.0000\n",
      "  page_16.jpg: 14.0000\n",
      "  page_18.jpg: 14.0000\n",
      "  page_40.jpg: 14.0000\n",
      "  page_69.jpg: 13.9375\n",
      "  page_6.jpg: 13.9375\n",
      "  page_55.jpg: 13.8750\n",
      "  page_15.jpg: 13.8750\n",
      "  page_51.jpg: 13.8125\n",
      "  page_53.jpg: 13.8125\n",
      "  page_8.jpg: 13.6250\n",
      "  page_49.jpg: 13.6250\n",
      "  page_61.jpg: 13.5625\n",
      "  page_2.jpg: 13.5000\n",
      "  page_13.jpg: 13.5000\n",
      "  page_43.jpg: 13.3750\n",
      "  page_44.jpg: 13.3750\n",
      "  page_77.jpg: 13.1875\n",
      "  page_76.jpg: 13.0625\n",
      "  page_7.jpg: 13.0000\n",
      "  page_80.jpg: 12.6875\n",
      "  page_4.jpg: 12.0625\n",
      "  page_56.jpg: 10.7500\n",
      "\n",
      "Query: ما هو إجمالي عائد المساهمين لصندوق الاستثمارات العامة منذ بداية برنامج تحقيق الرؤية؟\n",
      "  page_1.jpg: 16.7500\n",
      "  page_23.jpg: 14.8125\n",
      "  page_52.jpg: 14.5625\n",
      "  page_5.jpg: 14.5000\n",
      "  page_22.jpg: 14.5000\n",
      "  page_6.jpg: 14.5000\n",
      "  page_35.jpg: 14.5000\n",
      "  page_66.jpg: 14.3750\n",
      "  page_79.jpg: 14.3125\n",
      "  page_75.jpg: 14.3125\n",
      "  page_27.jpg: 14.2500\n",
      "  page_10.jpg: 14.2500\n",
      "  page_14.jpg: 14.1875\n",
      "  page_20.jpg: 14.1875\n",
      "  page_78.jpg: 14.1250\n",
      "  page_21.jpg: 14.1250\n",
      "  page_65.jpg: 14.0625\n",
      "  page_74.jpg: 14.0000\n",
      "  page_48.jpg: 14.0000\n",
      "  page_3.jpg: 13.9375\n",
      "  page_68.jpg: 13.9375\n",
      "  page_26.jpg: 13.9375\n",
      "  page_12.jpg: 13.9375\n",
      "  page_42.jpg: 13.8750\n",
      "  page_72.jpg: 13.8750\n",
      "  page_73.jpg: 13.8750\n",
      "  page_25.jpg: 13.8750\n",
      "  page_29.jpg: 13.8750\n",
      "  page_46.jpg: 13.8750\n",
      "  page_36.jpg: 13.8750\n",
      "  page_60.jpg: 13.8125\n",
      "  page_24.jpg: 13.8125\n",
      "  page_11.jpg: 13.8125\n",
      "  page_50.jpg: 13.8125\n",
      "  page_57.jpg: 13.6875\n",
      "  page_45.jpg: 13.6875\n",
      "  page_70.jpg: 13.6875\n",
      "  page_51.jpg: 13.6875\n",
      "  page_34.jpg: 13.6875\n",
      "  page_80.jpg: 13.6250\n",
      "  page_58.jpg: 13.6250\n",
      "  page_41.jpg: 13.6250\n",
      "  page_31.jpg: 13.6250\n",
      "  page_28.jpg: 13.5625\n",
      "  page_30.jpg: 13.5625\n",
      "  page_19.jpg: 13.5625\n",
      "  page_38.jpg: 13.5625\n",
      "  page_71.jpg: 13.5000\n",
      "  page_67.jpg: 13.5000\n",
      "  page_17.jpg: 13.4375\n",
      "  page_37.jpg: 13.4375\n",
      "  page_39.jpg: 13.4375\n",
      "  page_77.jpg: 13.3750\n",
      "  page_62.jpg: 13.3750\n",
      "  page_54.jpg: 13.3750\n",
      "  page_59.jpg: 13.3125\n",
      "  page_64.jpg: 13.3125\n",
      "  page_9.jpg: 13.3125\n",
      "  page_47.jpg: 13.3125\n",
      "  page_32.jpg: 13.2500\n",
      "  page_15.jpg: 13.2500\n",
      "  page_40.jpg: 13.2500\n",
      "  page_16.jpg: 13.1875\n",
      "  page_18.jpg: 13.1875\n",
      "  page_69.jpg: 13.1250\n",
      "  page_63.jpg: 13.1250\n",
      "  page_33.jpg: 13.1250\n",
      "  page_2.jpg: 13.0000\n",
      "  page_61.jpg: 12.9375\n",
      "  page_53.jpg: 12.8125\n",
      "  page_55.jpg: 12.6250\n",
      "  page_44.jpg: 12.5000\n",
      "  page_43.jpg: 12.4375\n",
      "  page_8.jpg: 12.3750\n",
      "  page_13.jpg: 12.3750\n",
      "  page_49.jpg: 12.1875\n",
      "  page_76.jpg: 12.0625\n",
      "  page_7.jpg: 12.0000\n",
      "  page_4.jpg: 11.8750\n",
      "  page_56.jpg: 10.7500\n",
      "\n",
      "Script completed in 59.40 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def process_image_directory(image_dir, queries, base_model=\"vidore/colqwen2-base\", adapter_model=\"vidore/colqwen2-v0.1\"):\n",
    "    \"\"\"\n",
    "    Process all images in a directory with the ColQwen2 model.\n",
    "    \n",
    "    Args:\n",
    "        image_dir (str): Path to directory containing images\n",
    "        queries (list): List of text queries to score against the images\n",
    "        base_model (str): HuggingFace model identifier for the base model\n",
    "        adapter_model (str): HuggingFace model identifier for the adapter\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results with scores for each query-image pair\n",
    "    \"\"\"\n",
    "    # Load base model and adapter\n",
    "    print(f\"Loading base model from {base_model}...\")\n",
    "    model = ColQwen2.from_pretrained(\n",
    "        base_model,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "    ).eval()\n",
    "    \n",
    "    # Load adapter\n",
    "    print(f\"Loading adapter from {adapter_model}...\")\n",
    "    try:\n",
    "        model.load_adapter(adapter_model)\n",
    "        print(\"Adapter loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load adapter: {e}\")\n",
    "        print(\"Continuing with base model only...\")\n",
    "    \n",
    "    processor = ColQwen2Processor.from_pretrained(base_model)\n",
    "    print(\"Model and processor loaded successfully\")\n",
    "    \n",
    "    # Collect all valid images from directory\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']\n",
    "    \n",
    "    print(f\"Loading images from {image_dir}...\")\n",
    "    for filename in os.listdir(image_dir):\n",
    "        file_path = os.path.join(image_dir, filename)\n",
    "        file_ext = os.path.splitext(filename)[1].lower()\n",
    "        \n",
    "        if os.path.isfile(file_path) and file_ext in valid_extensions:\n",
    "            try:\n",
    "                img = Image.open(file_path).convert('RGB')\n",
    "                images.append(img)\n",
    "                image_paths.append(filename)\n",
    "                print(f\"Loaded image: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    if not images:\n",
    "        print(\"No valid images found in directory\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing {len(images)} images with {len(queries)} queries...\")\n",
    "    \n",
    "    print(\"Processing queries...\")\n",
    "    query_embeddings = []\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        try:\n",
    "            batch_queries = processor.process_queries([query])\n",
    "            batch_queries = {k: v.to(model.device) for k, v in batch_queries.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = model(**batch_queries)\n",
    "                query_embeddings.append(embedding)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing query: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Process images one at a time\n",
    "    print(\"Processing images...\")\n",
    "    image_embeddings = []\n",
    "    successful_images = []\n",
    "    \n",
    "    for i, (img, img_path) in enumerate(zip(images, image_paths)):\n",
    "        # print(f\"  Processing image {i+1}/{len(images)}: {img_path}\")\n",
    "        try:\n",
    "            # Process a single image\n",
    "            batch_images = processor.process_images([img])\n",
    "            batch_images = {k: v.to(model.device) for k, v in batch_images.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = model(**batch_images)\n",
    "                image_embeddings.append(embedding)\n",
    "                successful_images.append(img_path)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing image {img_path}: {e}\")\n",
    "    \n",
    "    if not image_embeddings:\n",
    "        print(\"No image embeddings were processed successfully.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    print(\"Calculating similarity scores...\")\n",
    "    results = {}\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        query_scores = {}\n",
    "        query_emb = query_embeddings[i]\n",
    "        \n",
    "        for j, img_path in enumerate(successful_images):\n",
    "            img_emb = image_embeddings[j]\n",
    "            \n",
    "            # Calculate cosine similarity (using score_multi_vector from processor)\n",
    "            scores = processor.score_multi_vector(query_emb, img_emb)\n",
    "            \n",
    "            # Get the score (should be a single value)\n",
    "            score = scores[0, 0].item()\n",
    "            query_scores[img_path] = score\n",
    "        \n",
    "        results[query] = query_scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your image directory and queries\n",
    "    image_directory = \"pif_ar\"\n",
    "    queries = [\n",
    "            \"ما هو إجمالي الأصول المدارة لصندوق الاستثمارات العامة حتى عام 2023؟\",\n",
    "    \"ما هو إجمالي عائد المساهمين لصندوق الاستثمارات العامة منذ بداية برنامج تحقيق الرؤية؟\",\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Process the directory\n",
    "    results = process_image_directory(image_directory, queries)\n",
    "    \n",
    "    # Display results\n",
    "    if results:\n",
    "        for query, scores in results.items():\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            # Sort images by score (highest first)\n",
    "            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            for img_name, score in sorted_scores:\n",
    "                print(f\"  {img_name}: {score:.4f}\")\n",
    "    else:\n",
    "        print(\"No results were obtained.\")\n",
    "\n",
    "    total_execution_time = time.time() - start_time\n",
    "    print(f\"\\nScript completed in {total_execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317965e0-f8b7-4d36-a3a9-47b1b671e6de",
   "metadata": {},
   "source": [
    "# Lets combine qwenvl with retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16eed51a-48cc-4900-84a0-b90e366acfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images to process with QwenVL\n",
      "Loading QwenVL model from Qwen/Qwen2.5-VL-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a77efdd36d483ab9eee30de7cb9e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be44657527254409a8906d7a21de2c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41c135f8bca4bf79f5e80c51d26c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c128d424b7e44248218c2827d091741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2583b3a2378438eab3240a3b0377268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7aa2699231497e879dbaa3c8f7b169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af220fd2c594e628b1f99af3fa94d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QwenVL model and processor loaded successfully\n",
      "Processing image: page_23.jpg\n",
      "Processing image: page_22.jpg\n",
      "\n",
      "===== OCR RESULTS =====\n",
      "\n",
      "Image 1: page_23.jpg\n",
      "----------------------------------------\n",
      "Here is the extracted text from the image:\n",
      "\n",
      "---\n",
      "\n",
      "**هادألا ىلع ةماع ةرخآ | ةينيطسلف**\n",
      "\n",
      "م2025 ماعل ةماعلا تارامثتسلاا قودنص هادأ\n",
      "\n",
      "**ذنم نيمهاسملا دئاع يلامإجإ عباتلا ةيؤرلا قيقحت مهأرب هدب ةماعلا تارامثتسلاا قودنصل (ًانوتس) %8.7**\n",
      "\n",
      "**ماعلا قوسلا قودنصلا ميقن | ملاك23**\n",
      "\n",
      "**ماعملا اًضيوعو تاغايضلا نم ةءاسو ةعومجم ىلع تارامثتسلاا عيزوت مت .يلبأك تاعاطقلاب ةيبلطلا**\n",
      "\n",
      "**%9.4 تامولعملا ةيبلط**\n",
      "**%17.0 راطملا**\n",
      "**%23.1 ةيلحلما**\n",
      "**%5.5 ةيملعلا قفوتملا**\n",
      "**%6.9 تامولعملا**\n",
      "**%7.3 ةيملعلا**\n",
      "**%2.5 ةيلاحلا ةيجاهنملا عمتلمجا**\n",
      "**%3.1 ةيكلملا**\n",
      "**%4.6 ةيملعلا دوجولا**\n",
      "**%18.9 ةيملعلا رفوت**\n",
      "**%0.4 تامولعملا**\n",
      "**%1.2 ةيملعلا ةيجرد ةيجاهنملا عمتلمجا**\n",
      "\n",
      "---\n",
      "\n",
      "This text provides detailed statistics about various categories related to the year 2025, including percentages for different groups and activities.\n",
      "----------------------------------------\n",
      "\n",
      "Image 2: page_22.jpg\n",
      "----------------------------------------\n",
      "Here is the extracted text from the provided image:\n",
      "\n",
      "---\n",
      "\n",
      "**م2023 يف ءادلأا ىلع ةماع ةرظن**\n",
      "\n",
      "| يدوعس لاجر | ةرادلإا تحت لوصلاا يلامجلإا | يدوعس لاجر | ةرادلإا تحت لوصلاا يلامجلإا |\n",
      "| --- | --- | --- | --- |\n",
      "| رايلم | ةرادلإا تحت لوصلاا عيزوت | رايلم | ةرادلإا تحت لوصلاا عيزوت |\n",
      "| 586 | 2,871 | 2,194 | 1,980 |\n",
      "| ةيملاك تارامثتسا | ةيملاك تارامثتسا ةقاطم | ةيملاك قاوسلاا ىلع تارامثتسا عجارم ةقاط | ةيملاك قاوسلاا ىلع تارامثتسا عجارم ةقاط |\n",
      "| 9% | 9% | 5% | 7% |\n",
      "| ةيرئاسيسلا ةيملاكلا تارامثتسا ةقاطم | ةعومجملا ةيملاكلا تارامثتسا ةقاطم | تاكولملا ىلع تارامثتسا ةقاطم | تاكولملا ىلع تارامثتسا ةقاطم |\n",
      "| 249 | 156 | 777 | 710 |\n",
      "| ةعومجملا ةيملاكلا تارامثتسا ةقاطم | ةيملاكلا قاوسلاا ىلع تارامثتسا عجارم ةقاط | ريوطت ىلإ ةعومجملا تارامثتسا ةقاطم | ريوطت ىلإ ةعومجملا تارامثتسا ةقاطم |\n",
      "| 109 | 190 | 670 | 318 |\n",
      "| ةيملاكلا قاوسلاا ىلع تارامثتسا عجارم ةقاط | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم |\n",
      "| 169 | 943 | 204 | 170 |\n",
      "| ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | كيكلا ةيئانثوألا عيضاخلا ةقاطم | كيكلا ةيئانثوألا عيضاخلا ةقاطم |\n",
      "| 710 | 255 | 121 | 35 |\n",
      "| ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم | ةيئانثوألا عيضاخلا ىلإ تارامثتسا ةقاطم |\n",
      "| 206 | 241 | 197 | 197 |\n",
      "\n",
      "**م2023 ماعلا قودنص قودنصلا ميقن**\n",
      "\n",
      "---\n",
      "\n",
      "This table provides data on the number of people in different categories for the years 2021, 2022, and 2023, categorized by various factors such as gender, age group, and type of service. The data is presented in Arabic and includes numerical values and percentages.\n",
      "----------------------------------------\n",
      "\n",
      "Results saved to ocr_results.txt\n",
      "\n",
      "Script completed in 55.45 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "from typing import List\n",
    "import base64\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# This function is no longer needed as we're directly specifying image paths\n",
    "\n",
    "class QwenVLProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "        min_pixels: int = 128*16*16,\n",
    "        max_pixels: int = 1024*16*16,\n",
    "        cache_dir: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the QwenVL processor with custom configuration.\n",
    "        \"\"\"\n",
    "        # Configure CUDA memory allocation\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Loading QwenVL model from {model_name}...\")\n",
    "        # Load model and assign to self\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            use_cache=True,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # Load processor and assign to self\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            min_pixels=min_pixels,\n",
    "            max_pixels=max_pixels,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        print(\"QwenVL model and processor loaded successfully\")\n",
    "\n",
    "    def _encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode a local image file to base64.\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def prepare_messages(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        prompt: str\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Prepare messages for the model using local image paths.\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "\n",
    "        messages = []\n",
    "        for path in image_paths:\n",
    "            encoded_image = self._encode_image(path)\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": encoded_image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    def process_images(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 2000,\n",
    "        temperature: float = 0.1,\n",
    "        top_p: float = 0.9\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process local images with the given prompt.\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        # Process one image at a time to avoid memory issues\n",
    "        for image_path in image_paths:\n",
    "            print(f\"Processing image: {os.path.basename(image_path)}\")\n",
    "            messages = self.prepare_messages(image_path, prompt)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                generated_ids_trimmed = [\n",
    "                    out_ids[len(in_ids):]\n",
    "                    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                ]\n",
    "\n",
    "                output_text = self.processor.batch_decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False\n",
    "                )\n",
    "                \n",
    "                results.append(output_text[0])\n",
    "                \n",
    "                # Clear cache after each image\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set your image directory\n",
    "    image_directory = \"pif_ar\"  # Base directory containing images\n",
    "    \n",
    "    \n",
    "    image_filenames = [\n",
    "    \"page_23.jpg\",\n",
    "    \"page_22.jpg\",\n",
    "    ]\n",
    "    \n",
    "    # Generate full paths\n",
    "    image_paths = [os.path.join(image_directory, filename) for filename in image_filenames]\n",
    "    \n",
    "    # Verify images exist\n",
    "    valid_image_paths = []\n",
    "    for path in image_paths:\n",
    "        if os.path.isfile(path):\n",
    "            valid_image_paths.append(path)\n",
    "        else:\n",
    "            print(f\"Warning: Image not found: {path}\")\n",
    "    \n",
    "    if not valid_image_paths:\n",
    "        print(\"No valid images found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(valid_image_paths)} images to process with QwenVL\")\n",
    "    \n",
    "    # Initialize QwenVL processor\n",
    "    processor = QwenVLProcessor()\n",
    "    \n",
    "    # OCR prompt\n",
    "    ocr_prompt = \"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "                   and in great precision. Given these images extract every detail of text in an organized format.\n",
    "                   Include all text visible in the image, preserving the structure where possible.\"\"\"\n",
    "    \n",
    "    # Process images with QwenVL for OCR\n",
    "    results = processor.process_images(valid_image_paths, prompt=ocr_prompt)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n===== OCR RESULTS =====\")\n",
    "    for i, (image_path, ocr_text) in enumerate(zip(valid_image_paths, results)):\n",
    "        print(f\"\\nImage {i+1}: {os.path.basename(image_path)}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(ocr_text)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(\"ocr_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (image_path, ocr_text) in enumerate(zip(valid_image_paths, results)):\n",
    "            f.write(f\"\\nImage {i+1}: {os.path.basename(image_path)}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(ocr_text + \"\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nResults saved to ocr_results.txt\")\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    print(f\"\\nScript completed in {total_execution_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5979b43b-ed59-46fe-9791-edb35db80351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images to process with QwenVL\n",
      "Loading QwenVL model from Qwen/Qwen2.5-VL-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651b036f36974736abd2899b5530e7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QwenVL model and processor loaded successfully\n",
      "Processing image: page_23.jpg\n",
      "Processing image: page_22.jpg\n",
      "\n",
      "===== OCR RESULTS =====\n",
      "\n",
      "Image 1: page_23.jpg\n",
      "----------------------------------------\n",
      "الاستراتيجية | نظرة عامة على الأداء\n",
      "\n",
      "أداء صندوق الاستثمارات العامة لعام 2023م\n",
      "\n",
      "تم توزيع الاستثمارات على مجموعة واسعة من الصناعات، ووفقاً للمعايير العالمية للقطاعات، كما يلي:\n",
      "\n",
      "تقنية المعلومات %9.4\n",
      "العقار %17.0\n",
      "الطاقة %23.1\n",
      "المراقبة العامة %5.5\n",
      "الاتصالات %6.9\n",
      "المالية %7.3\n",
      "السلع الاستهلاكية الكاملة %2.5\n",
      "الصناعات %3.1\n",
      "المواد الأساسية %4.6\n",
      "غير مصنفة %18.9\n",
      "الصحة %0.4\n",
      "السلع الاستهلاكية الأساسية %1.2\n",
      "\n",
      "إجمالي عائد المساهمين منذ بدء برنامج تحقيق الرؤية التابع لصندوق الاستثمارات العامة (سنويًا) %8.7\n",
      "\n",
      "1- إجمالي عائد المساهمين منذ بدء برنامج تحقيق الرؤية في 30 سبتمبر 2017م حتى نهاية عام 2023م (على أساس سنوي).\n",
      "2- ممثل الصناديق غير المصنفة / استثمارات لديها أصول متعددة، والنقد، وحسابات تحت الطلب، والودائع الآجلة وصناديق أسواق النقد.\n",
      "\n",
      "تقرير الصندوق السنوي لعام 2023م | 44\n",
      "----------------------------------------\n",
      "\n",
      "Image 2: page_22.jpg\n",
      "----------------------------------------\n",
      "الإستراتيجية | نظرة عامة على الأداء\n",
      "\n",
      "نظرة عامة على الأداء في 2023م\n",
      "\n",
      "إجمالي الأصول تحت الإدارة1\n",
      "مليار ريال سعودي 2،871\n",
      "\n",
      "توزيع الأصول المدارة1\n",
      "مليار ريال سعودي 586\n",
      "مليار ريال سعودي 2،194\n",
      "مليار ريال سعودي 72\n",
      "\n",
      "استثمارات عالمية\n",
      "محفظة الاستثمارات العالمية الإستراتيجية 249\n",
      "محفظة الاستثمارات العالمية المتنوعة 146\n",
      "برنامج الاستثمارات في الأسواق العالمية العامة 190\n",
      "محفظة الاستثمارات في الشركات السعودية 777\n",
      "محفظة الاستثمارات الهادفة إلى تطوير القطاعات الوعيدة وتنميتها 943\n",
      "محفظة الاستثمارات في المشاريع العقارية ومشاريع تطوير البنية التحتية السعودية 233\n",
      "محفظة المشاريع السعودية الكبرى 241\n",
      "محفظة الخزينة (محفظة غير استثمارية) 72\n",
      "\n",
      "مليار ريال سعودي 2،234\n",
      "مليار ريال سعودي 1،980\n",
      "\n",
      "محفظة الاستثمارات العالمية الإستراتيجية 390\n",
      "محفظة الاستثمارات العالمية المتنوعة 66\n",
      "برنامج الاستثمارات في الأسواق العالمية العامة 121\n",
      "محفظة الاستثمارات في الشركات السعودية 483\n",
      "محفظة الاستثمارات الهادفة إلى تطوير القطاعات الوعيدة وتنميتها 318\n",
      "محفظة الاستثمارات في المشاريع العقارية ومشاريع تطوير البنية التحتية السعودية 170\n",
      "محفظة المشاريع السعودية الكبرى 35\n",
      "محفظة الخزينة (محفظة غير استثمارية) 397\n",
      "\n",
      "مع إدارة 2،871 مليار ريال سعودي، تم تنويع الاستثمارات باستثمارات دولية بنسبة 20% والاستثمارات المحلية بنسبة 79% (تشمل الخزينة) من المجموع فيما تعود نسبة 1% المتبقية إلى المبالغ المستردة من الجهات الأخرى.\n",
      "\n",
      "1- الإجمالي قد لا يكون منتمائًا لأسباب التقرير.\n",
      "2- يتضمن إجمالي قيمة الأصول مبلغ مسترد من الجهات الأخرى بقيمة 20 مليار ريال.\n",
      "3- يتضمن إجمالي قيمة الأصول مبلغ مسترد من الجهات الأخرى بقيمة 5 مليار ريال.\n",
      "\n",
      "تقرير الصندوق السنوي لعام 2023م\n",
      "----------------------------------------\n",
      "\n",
      "Results saved to ocr_results.txt\n",
      "\n",
      "Script completed in 36.90 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "from typing import List\n",
    "import base64\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "class QwenVLProcessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "        device: str = \"cuda\",\n",
    "        min_pixels: int = 128*16*16,\n",
    "        max_pixels: int = 1600*40*40,\n",
    "        cache_dir: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the QwenVL processor with custom configuration.\n",
    "        \"\"\"\n",
    "        # Configure CUDA memory allocation\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Loading QwenVL model from {model_name}...\")\n",
    "        # Load model and assign to self\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            use_cache=True,\n",
    "            cache_dir=cache_dir,\n",
    "        )\n",
    "\n",
    "        # Load processor and assign to self\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            model_name,\n",
    "            min_pixels=min_pixels,\n",
    "            max_pixels=max_pixels,\n",
    "            use_fast=True\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        print(\"QwenVL model and processor loaded successfully\")\n",
    "\n",
    "    def _encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Encode a local image file to base64.\n",
    "        \"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        return f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "    def prepare_messages(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        prompt: str\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Prepare messages for the model using local image paths.\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "\n",
    "        messages = []\n",
    "        for path in image_paths:\n",
    "            encoded_image = self._encode_image(path)\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": encoded_image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            })\n",
    "        return messages\n",
    "\n",
    "    def process_images(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 2000,\n",
    "        temperature: float = 0.1,\n",
    "        top_p: float = 0.9\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process local images with the given prompt.\n",
    "        \"\"\"\n",
    "        if isinstance(image_paths, str):\n",
    "            image_paths = [image_paths]\n",
    "            \n",
    "        results = []\n",
    "        \n",
    "        # Process one image at a time to avoid memory issues\n",
    "        for image_path in image_paths:\n",
    "            print(f\"Processing image: {os.path.basename(image_path)}\")\n",
    "            messages = self.prepare_messages(image_path, prompt)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                text = self.processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "\n",
    "                image_inputs, video_inputs = process_vision_info(messages)\n",
    "                inputs = self.processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "                generated_ids = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                generated_ids_trimmed = [\n",
    "                    out_ids[len(in_ids):]\n",
    "                    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                ]\n",
    "\n",
    "                output_text = self.processor.batch_decode(\n",
    "                    generated_ids_trimmed,\n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=False\n",
    "                )\n",
    "                \n",
    "                results.append(output_text[0])\n",
    "                \n",
    "                # Clear cache after each image\n",
    "                if self.device == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set your image directory\n",
    "    image_directory = \"pif_ar\"  # Base directory containing images\n",
    "    \n",
    "    \n",
    "    image_filenames = [\n",
    "    \"page_23.jpg\",\n",
    "    \"page_22.jpg\",\n",
    "    ]\n",
    "    \n",
    "    # Generate full paths\n",
    "    image_paths = [os.path.join(image_directory, filename) for filename in image_filenames]\n",
    "    \n",
    "    # Verify images exist\n",
    "    valid_image_paths = []\n",
    "    for path in image_paths:\n",
    "        if os.path.isfile(path):\n",
    "            valid_image_paths.append(path)\n",
    "        else:\n",
    "            print(f\"Warning: Image not found: {path}\")\n",
    "    \n",
    "    if not valid_image_paths:\n",
    "        print(\"No valid images found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(valid_image_paths)} images to process with QwenVL\")\n",
    "    \n",
    "    # Initialize QwenVL processor\n",
    "    processor = QwenVLProcessor()\n",
    "    \n",
    "    # OCR prompt\n",
    "    ocr_prompt = \"\"\"You are an expert OCR model who can read and interpret hard images in details\n",
    "                   and in great precision. Given these images extract every detail of text in an organized format.\n",
    "                   Include all text visible in the image, preserving the structure where possible .. generate in arabic text\"\"\"\n",
    "    \n",
    "    # Process images with QwenVL for OCR\n",
    "    results = processor.process_images(valid_image_paths, prompt=ocr_prompt)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n===== OCR RESULTS =====\")\n",
    "    for i, (image_path, ocr_text) in enumerate(zip(valid_image_paths, results)):\n",
    "        print(f\"\\nImage {i+1}: {os.path.basename(image_path)}\")\n",
    "        print(\"-\" * 40)\n",
    "        print(ocr_text)\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(\"ocr_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (image_path, ocr_text) in enumerate(zip(valid_image_paths, results)):\n",
    "            f.write(f\"\\nImage {i+1}: {os.path.basename(image_path)}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(ocr_text + \"\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nResults saved to ocr_results.txt\")\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    print(f\"\\nScript completed in {total_execution_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a963051-c2ce-4254-a151-09a1b6e0441d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
